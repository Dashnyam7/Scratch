{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMwaSc4y6aY3CbufmfHgsu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dashnyam7/Scratch/blob/main/Linear_regression_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1] Hypothetical function"
      ],
      "metadata": {
        "id": "BzOqcadknccr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6LavBL5knWzS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 2] The gradient descent method"
      ],
      "metadata": {
        "id": "pU8eObB3pYpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, error):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - error[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return\n"
      ],
      "metadata": {
        "id": "TnAWsZgrpiuW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 3] Presumption"
      ],
      "metadata": {
        "id": "vUWTFbnGpcvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y"
      ],
      "metadata": {
        "id": "rXT9Pkh8rDTN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 4]  Mean squared error"
      ],
      "metadata": {
        "id": "RTfkShIXuHYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse"
      ],
      "metadata": {
        "id": "scmjINLDuIyH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 5] Objective function"
      ],
      "metadata": {
        "id": "TsZw-XbXvMTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self,y_pred, y):\n",
        "        loss = self.MSE(pred, y)/2\n",
        "        return loss"
      ],
      "metadata": {
        "id": "xzF9PGULvQ-y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 6] Learning and Estimation"
      ],
      "metadata": {
        "id": "TapFPhUgvnxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        #self.loss = np.zeros(self.iter)\n",
        "        #self.val_loss = np.zeros(self.iter)\n",
        "        self.theta = np.array([])\n",
        "        self.loss = np.array([])\n",
        "        self.val_loss = np.array([])\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        y_pred = X @ self.theta\n",
        "        return y_pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      y_pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (y_pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)   \n",
        "\n",
        "    def MSE(self, y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / y.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self, y_pred, y):\n",
        "        loss = self.MSE(y_pred, y)/2\n",
        "        return loss \n",
        "        \n",
        "    def fit(self, X, y, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"        \n",
        "        if self.no_bias == True:\n",
        "            no_bias = np.ones((X.shape[0], 1))\n",
        "            X = np.hstack((no_bias, X))\n",
        "            no_bias = np.ones((X_val.shape[0], 1))\n",
        "            X_val = np.hstack((no_bias, X_val))\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "        for i in range(self.iter):\n",
        "            pred = self._linear_hypothesis(X)\n",
        "            pred_val = self._linear_hypothesis(X_val)\n",
        "            self._gradient_descent(X, y)\n",
        "            loss = self._loss_func(pred, y)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            loss_val = self._loss_func(pred_val, y_val)\n",
        "            self.val_loss = np.append(self.val_loss, loss_val)\n",
        "            if self.verbose:\n",
        "                #Output learning process when verbose is set to True\n",
        "                print('The {}th training loss is{}'.format(i,loss))    \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "Y1qcCXdXv7fw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "UjDh3aZNwi6m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
        "slr.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEfB1FOewmpd",
        "outputId": "1916bec0-2159-4a0b-b56c-c1abca18d55e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 0th training loss is19606034574.587757\n",
            "The 1th training loss is19229979221.88102\n",
            "The 2th training loss is18861598746.05494\n",
            "The 3th training loss is18500735790.256027\n",
            "The 4th training loss is18147236240.567078\n",
            "The 5th training loss is17800949158.78915\n",
            "The 6th training loss is17461726716.625668\n",
            "The 7th training loss is17129424131.239222\n",
            "The 8th training loss is16803899602.15219\n",
            "The 9th training loss is16485014249.46301\n",
            "The 10th training loss is16172632053.350466\n",
            "The 11th training loss is15866619794.838915\n",
            "The 12th training loss is15566846997.798061\n",
            "The 13th training loss is15273185872.151323\n",
            "The 14th training loss is14985511258.267422\n",
            "The 15th training loss is14703700572.510427\n",
            "The 16th training loss is14427633753.92393\n",
            "The 17th training loss is14157193212.025515\n",
            "The 18th training loss is13892263775.688366\n",
            "The 19th training loss is13632732643.087084\n",
            "The 20th training loss is13378489332.685495\n",
            "The 21th training loss is13129425635.244593\n",
            "The 22th training loss is12885435566.829226\n",
            "The 23th training loss is12646415322.7926\n",
            "The 24th training loss is12412263232.71816\n",
            "The 25th training loss is12182879716.298714\n",
            "The 26th training loss is11958167240.13326\n",
            "The 27th training loss is11738030275.422256\n",
            "The 28th training loss is11522375256.542526\n",
            "The 29th training loss is11311110540.483389\n",
            "The 30th training loss is11104146367.12601\n",
            "The 31th training loss is10901394820.348309\n",
            "The 32th training loss is10702769789.938135\n",
            "The 33th training loss is10508186934.297861\n",
            "The 34th training loss is10317563643.923775\n",
            "The 35th training loss is10130819005.644106\n",
            "The 36th training loss is9947873767.59983\n",
            "The 37th training loss is9768650304.952675\n",
            "The 38th training loss is9593072586.305202\n",
            "The 39th training loss is9421066140.818\n",
            "The 40th training loss is9252558026.009508\n",
            "The 41th training loss is9087476796.224108\n",
            "The 42th training loss is8925752471.754642\n",
            "The 43th training loss is8767316508.60556\n",
            "The 44th training loss is8612101768.883442\n",
            "The 45th training loss is8460042491.801694\n",
            "The 46th training loss is8311074265.286654\n",
            "The 47th training loss is8165133998.172543\n",
            "The 48th training loss is8022159892.97294\n",
            "The 49th training loss is7882091419.216821\n",
            "The 50th training loss is7744869287.337284\n",
            "The 51th training loss is7610435423.101521\n",
            "The 52th training loss is7478732942.570697\n",
            "The 53th training loss is7349706127.5786915\n",
            "The 54th training loss is7223300401.718894\n",
            "The 55th training loss is7099462306.828454\n",
            "The 56th training loss is6978139479.959619\n",
            "The 57th training loss is6859280630.827995\n",
            "The 58th training loss is6742835519.727817\n",
            "The 59th training loss is6628754935.904459\n",
            "The 60th training loss is6516990676.374705\n",
            "The 61th training loss is6407495525.18541\n",
            "The 62th training loss is6300223233.101447\n",
            "The 63th training loss is6195128497.713989\n",
            "The 64th training loss is6092166943.960379\n",
            "The 65th training loss is5991295105.047009\n",
            "The 66th training loss is5892470403.766843\n",
            "The 67th training loss is5795651134.203329\n",
            "The 68th training loss is5700796443.812714\n",
            "The 69th training loss is5607866315.876821\n",
            "The 70th training loss is5516821552.318662\n",
            "The 71th training loss is5427623756.873255\n",
            "The 72th training loss is5340235318.606325\n",
            "The 73th training loss is5254619395.773613\n",
            "The 74th training loss is5170739900.013738\n",
            "The 75th training loss is5088561480.867649\n",
            "The 76th training loss is5008049510.617911\n",
            "The 77th training loss is4929170069.441152\n",
            "The 78th training loss is4851889930.867173\n",
            "The 79th training loss is4776176547.538352\n",
            "The 80th training loss is4701998037.263098\n",
            "The 81th training loss is4629323169.357247\n",
            "The 82th training loss is4558121351.267432\n",
            "The 83th training loss is4488362615.470548\n",
            "The 84th training loss is4420017606.643597\n",
            "The 85th training loss is4353057569.098301\n",
            "The 86th training loss is4287454334.474979\n",
            "The 87th training loss is4223180309.6903043\n",
            "The 88th training loss is4160208465.1336856\n",
            "The 89th training loss is4098512323.107099\n",
            "The 90th training loss is4038065946.5033445\n",
            "The 91th training loss is3978843927.717738\n",
            "The 92th training loss is3920821377.7884502\n",
            "The 93th training loss is3863973915.7607036\n",
            "The 94th training loss is3808277658.2702236\n",
            "The 95th training loss is3753709209.3413644\n",
            "The 96th training loss is3700245650.3954935\n",
            "The 97th training loss is3647864530.4652452\n",
            "The 98th training loss is3596543856.6104016\n",
            "The 99th training loss is3546262084.5312066\n",
            "The 100th training loss is3496998109.3750305\n",
            "The 101th training loss is3448731256.7323804\n",
            "The 102th training loss is3401441273.818327\n",
            "The 103th training loss is3355108320.8355193\n",
            "The 104th training loss is3309712962.515018\n",
            "The 105th training loss is3265236159.831265\n",
            "The 106th training loss is3221659261.8875966\n",
            "The 107th training loss is3178963997.9687514\n",
            "The 108th training loss is3137132469.7569356\n",
            "The 109th training loss is3096147143.7080483\n",
            "The 110th training loss is3055990843.584755\n",
            "The 111th training loss is3016646743.1431727\n",
            "The 112th training loss is2978098358.969976\n",
            "The 113th training loss is2940329543.466826\n",
            "The 114th training loss is2903324477.979066\n",
            "The 115th training loss is2867067666.0657015\n",
            "The 116th training loss is2831543926.907755\n",
            "The 117th training loss is2796738388.852117\n",
            "The 118th training loss is2762636483.088107\n",
            "The 119th training loss is2729223937.453995\n",
            "The 120th training loss is2696486770.3708014\n",
            "The 121th training loss is2664411284.9007406\n",
            "The 122th training loss is2632984062.9277406\n",
            "The 123th training loss is2602191959.4575095\n",
            "The 124th training loss is2572022097.0346847\n",
            "The 125th training loss is2542461860.2746463\n",
            "The 126th training loss is2513498890.507624\n",
            "The 127th training loss is2485121080.5327835\n",
            "The 128th training loss is2457316569.480016\n",
            "The 129th training loss is2430073737.7772207\n",
            "The 130th training loss is2403381202.2208796\n",
            "The 131th training loss is2377227811.147822\n",
            "The 132th training loss is2351602639.7060685\n",
            "The 133th training loss is2326494985.2227173\n",
            "The 134th training loss is2301894362.666877\n",
            "The 135th training loss is2277790500.2056794\n",
            "The 136th training loss is2254173334.8514476\n",
            "The 137th training loss is2231033008.198158\n",
            "The 138th training loss is2208359862.2453313\n",
            "The 139th training loss is2186144435.3075686\n",
            "The 140th training loss is2164377458.007958\n",
            "The 141th training loss is2143049849.3536305\n",
            "The 142th training loss is2122152712.8917584\n",
            "The 143th training loss is2101677332.9443593\n",
            "The 144th training loss is2081615170.920262\n",
            "The 145th training loss is2061957861.702653\n",
            "The 146th training loss is2042697210.110654\n",
            "The 147th training loss is2023825187.4333951\n",
            "The 148th training loss is2005333928.0350919\n",
            "The 149th training loss is1987215726.0296707\n",
            "The 150th training loss is1969463032.0235062\n",
            "The 151th training loss is1952068449.9248674\n",
            "The 152th training loss is1935024733.8187056\n",
            "The 153th training loss is1918324784.9054255\n",
            "The 154th training loss is1901961648.502346\n",
            "The 155th training loss is1885928511.1065352\n",
            "The 156th training loss is1870218697.517774\n",
            "The 157th training loss is1854825668.0204074\n",
            "The 158th training loss is1839743015.6228695\n",
            "The 159th training loss is1824964463.353694\n",
            "The 160th training loss is1810483861.6128607\n",
            "The 161th training loss is1796295185.5773227\n",
            "The 162th training loss is1782392532.659616\n",
            "The 163th training loss is1768770120.0184503\n",
            "The 164th training loss is1755422282.1202183\n",
            "The 165th training loss is1742343468.3503723\n",
            "The 166th training loss is1729528240.6736462\n",
            "The 167th training loss is1716971271.342122\n",
            "The 168th training loss is1704667340.6501496\n",
            "The 169th training loss is1692611334.735168\n",
            "The 170th training loss is1680798243.423473\n",
            "The 171th training loss is1669223158.1200202\n",
            "The 172th training loss is1657881269.7413526\n",
            "The 173th training loss is1646767866.6907637\n",
            "The 174th training loss is1635878332.874842\n",
            "The 175th training loss is1625208145.7605295\n",
            "The 176th training loss is1614752874.47188\n",
            "The 177th training loss is1604508177.925688\n",
            "The 178th training loss is1594469803.0052006\n",
            "The 179th training loss is1584633582.7711272\n",
            "The 180th training loss is1574995434.7091777\n",
            "The 181th training loss is1565551359.0133913\n",
            "The 182th training loss is1556297436.904506\n",
            "The 183th training loss is1547229828.9826632\n",
            "The 184th training loss is1538344773.6137369\n",
            "The 185th training loss is1529638585.3486004\n",
            "The 186th training loss is1521107653.3746526\n",
            "The 187th training loss is1512748439.9989452\n",
            "The 188th training loss is1504557479.1622603\n",
            "The 189th training loss is1496531374.9835095\n",
            "The 190th training loss is1488666800.333823\n",
            "The 191th training loss is1480960495.4397306\n",
            "The 192th training loss is1473409266.5148358\n",
            "The 193th training loss is1466009984.4193902\n",
            "The 194th training loss is1458759583.3472118\n",
            "The 195th training loss is1451655059.5393763\n",
            "The 196th training loss is1444693470.0241344\n",
            "The 197th training loss is1437871931.3825228\n",
            "The 198th training loss is1431187618.5391366\n",
            "The 199th training loss is1424637763.577552\n",
            "The 200th training loss is1418219654.5798936\n",
            "The 201th training loss is1411930634.4900522\n",
            "The 202th training loss is1405768100.0000677\n",
            "The 203th training loss is1399729500.4592042\n",
            "The 204th training loss is1393812336.805253\n",
            "The 205th training loss is1388014160.5176063\n",
            "The 206th training loss is1382332572.591662\n",
            "The 207th training loss is1376765222.5341144\n",
            "The 208th training loss is1371309807.3787131\n",
            "The 209th training loss is1365964070.7220626\n",
            "The 210th training loss is1360725801.7790594\n",
            "The 211th training loss is1355592834.457562\n",
            "The 212th training loss is1350563046.4518986\n",
            "The 213th training loss is1345634358.354834\n",
            "The 214th training loss is1340804732.7876093\n",
            "The 215th training loss is1336072173.5476937\n",
            "The 216th training loss is1331434724.7738783\n",
            "The 217th training loss is1326890470.1283643\n",
            "The 218th training loss is1322437531.9954948\n",
            "The 219th training loss is1318074070.6967895\n",
            "The 220th training loss is1313798283.7219517\n",
            "The 221th training loss is1309608404.975519\n",
            "The 222th training loss is1305502704.0388393\n",
            "The 223th training loss is1301479485.4470596\n",
            "The 224th training loss is1297537087.980819\n",
            "The 225th training loss is1293673883.9723477\n",
            "The 226th training loss is1289888278.6256757\n",
            "The 227th training loss is1286178709.3506658\n",
            "The 228th training loss is1282543645.1105864\n",
            "The 229th training loss is1278981585.7829483\n",
            "The 230th training loss is1275491061.5333345\n",
            "The 231th training loss is1272070632.2019603\n",
            "The 232th training loss is1268718886.702699\n",
            "The 233th training loss is1265434442.4343219\n",
            "The 234th training loss is1262215944.7037075\n",
            "The 235th training loss is1259062066.160763\n",
            "The 236th training loss is1255971506.2448363\n",
            "The 237th training loss is1252942990.6423674\n",
            "The 238th training loss is1249975270.7555654\n",
            "The 239th training loss is1247067123.1818705\n",
            "The 240th training loss is1244217349.2039955\n",
            "The 241th training loss is1241424774.2903214\n",
            "The 242th training loss is1238688247.605441\n",
            "The 243th training loss is1236006641.5306423\n",
            "The 244th training loss is1233378851.1941233\n",
            "The 245th training loss is1230803794.0107532\n",
            "The 246th training loss is1228280409.2311728\n",
            "The 247th training loss is1225807657.5000474\n",
            "The 248th training loss is1223384520.4232914\n",
            "The 249th training loss is1221010000.14407\n",
            "The 250th training loss is1218683118.9274137\n",
            "The 251th training loss is1216402918.7532582\n",
            "The 252th training loss is1214168460.9177423\n",
            "The 253th training loss is1211978825.6425974\n",
            "The 254th training loss is1209833111.6924634\n",
            "The 255th training loss is1207730435.9999638\n",
            "The 256th training loss is1205669933.298392\n",
            "The 257th training loss is1203650755.761844\n",
            "The 258th training loss is1201672072.6526527\n",
            "The 259th training loss is1199733069.9759722\n",
            "The 260th training loss is1197832950.1413662\n",
            "The 261th training loss is1195970931.6312606\n",
            "The 262th training loss is1194146248.6761167\n",
            "The 263th training loss is1192358150.9361916\n",
            "The 264th training loss is1190605903.189749\n",
            "The 265th training loss is1188888785.0275948\n",
            "The 266th training loss is1187206090.5537968\n",
            "The 267th training loss is1185557128.09248\n",
            "The 268th training loss is1183941219.9005566\n",
            "The 269th training loss is1182357701.8862815\n",
            "The 270th training loss is1180805923.3335106\n",
            "The 271th training loss is1179285246.631547\n",
            "The 272th training loss is1177795047.010458\n",
            "The 273th training loss is1176334712.2817597\n",
            "The 274th training loss is1174903642.5843494\n",
            "The 275th training loss is1173501250.1355875\n",
            "The 276th training loss is1172126958.9874227\n",
            "The 277th training loss is1170780204.7874534\n",
            "The 278th training loss is1169460434.544829\n",
            "The 279th training loss is1168167106.400895\n",
            "The 280th training loss is1166899689.4044783\n",
            "The 281th training loss is1165657663.2917235\n",
            "The 282th training loss is1164440518.27039\n",
            "The 283th training loss is1163247754.8085117\n",
            "The 284th training loss is1162078883.427338\n",
            "The 285th training loss is1160933424.4984632\n",
            "The 286th training loss is1159810908.0450656\n",
            "The 287th training loss is1158710873.5471685\n",
            "The 288th training loss is1157632869.750839\n",
            "The 289th training loss is1156576454.481253\n",
            "The 290th training loss is1155541194.459538\n",
            "The 291th training loss is1154526665.1233258\n",
            "The 292th training loss is1153532450.4509351\n",
            "The 293th training loss is1152558142.7891073\n",
            "The 294th training loss is1151603342.684232\n",
            "The 295th training loss is1150667658.7169845\n",
            "The 296th training loss is1149750707.340307\n",
            "The 297th training loss is1148852112.7206664\n",
            "The 298th training loss is1147971506.5825272\n",
            "The 299th training loss is1147108528.0559628\n",
            "The 300th training loss is1146262823.5273523\n",
            "The 301th training loss is1145434046.4930964\n",
            "The 302th training loss is1144621857.4162872\n",
            "The 303th training loss is1143825923.5862815\n",
            "The 304th training loss is1143045918.9811077\n",
            "The 305th training loss is1142281524.132656\n",
            "The 306th training loss is1141532425.994595\n",
            "The 307th training loss is1140798317.8129535\n",
            "The 308th training loss is1140078898.9993243\n",
            "The 309th training loss is1139373875.006624\n",
            "The 310th training loss is1138682957.207368\n",
            "The 311th training loss is1138005862.774402\n",
            "The 312th training loss is1137342314.5640457\n",
            "The 313th training loss is1136692041.0015953\n",
            "The 314th training loss is1136054775.9691412\n",
            "The 315th training loss is1135430258.6956499\n",
            "The 316th training loss is1134818233.6492665\n",
            "The 317th training loss is1134218450.4317946\n",
            "The 318th training loss is1133630663.6753042\n",
            "The 319th training loss is1133054632.9408307\n",
            "The 320th training loss is1132490122.6191163\n",
            "The 321th training loss is1131936901.8333595\n",
            "The 322th training loss is1131394744.3439248\n",
            "The 323th training loss is1130863428.4549782\n",
            "The 324th training loss is1130342736.9230084\n",
            "The 325th training loss is1129832456.8671918\n",
            "The 326th training loss is1129332379.68157\n",
            "The 327th training loss is1128842300.9489977\n",
            "The 328th training loss is1128362020.3568304\n",
            "The 329th training loss is1127891341.6143098\n",
            "The 330th training loss is1127430072.3716192\n",
            "The 331th training loss is1126978024.1405723\n",
            "The 332th training loss is1126535012.216901\n",
            "The 333th training loss is1126100855.6041129\n",
            "The 334th training loss is1125675376.9388828\n",
            "The 335th training loss is1125258402.4179537\n",
            "The 336th training loss is1124849761.7265058\n",
            "The 337th training loss is1124449287.9679778\n",
            "The 338th training loss is1124056817.5952978\n",
            "The 339th training loss is1123672190.3435063\n",
            "The 340th training loss is1123295249.1637335\n",
            "The 341th training loss is1122925840.158511\n",
            "The 342th training loss is1122563812.5183876\n",
            "The 343th training loss is1122209018.4598234\n",
            "The 344th training loss is1121861313.1643348\n",
            "The 345th training loss is1121520554.7188678\n",
            "The 346th training loss is1121186604.0573735\n",
            "The 347th training loss is1120859324.9035616\n",
            "The 348th training loss is1120538583.7148054\n",
            "The 349th training loss is1120224249.6271827\n",
            "The 350th training loss is1119916194.4016185\n",
            "The 351th training loss is1119614292.3711188\n",
            "The 352th training loss is1119318420.3890634\n",
            "The 353th training loss is1119028457.7785435\n",
            "The 354th training loss is1118744286.2827187\n",
            "The 355th training loss is1118465790.016176\n",
            "The 356th training loss is1118192855.4172678\n",
            "The 357th training loss is1117925371.2014093\n",
            "The 358th training loss is1117663228.3153188\n",
            "The 359th training loss is1117406319.8921795\n",
            "The 360th training loss is1117154541.207703\n",
            "The 361th training loss is1116907789.637079\n",
            "The 362th training loss is1116665964.612793\n",
            "The 363th training loss is1116428967.583292\n",
            "The 364th training loss is1116196701.972484\n",
            "The 365th training loss is1115969073.1400516\n",
            "The 366th training loss is1115745988.3425663\n",
            "The 367th training loss is1115527356.695386\n",
            "The 368th training loss is1115313089.1353176\n",
            "The 369th training loss is1115103098.3840337\n",
            "The 370th training loss is1114897298.9122257\n",
            "The 371th training loss is1114695606.9044771\n",
            "The 372th training loss is1114497940.2248433\n",
            "The 373th training loss is1114304218.383129\n",
            "The 374th training loss is1114114362.5018382\n",
            "The 375th training loss is1113928295.283794\n",
            "The 376th training loss is1113745940.9804084\n",
            "The 377th training loss is1113567225.3605895\n",
            "The 378th training loss is1113392075.6802773\n",
            "The 379th training loss is1113220420.6525905\n",
            "The 380th training loss is1113052190.4185743\n",
            "The 381th training loss is1112887316.5185368\n",
            "The 382th training loss is1112725731.8639627\n",
            "The 383th training loss is1112567370.7099924\n",
            "The 384th training loss is1112412168.6284528\n",
            "The 385th training loss is1112260062.4814324\n",
            "The 386th training loss is1112110990.3953886\n",
            "The 387th training loss is1111964891.735773\n",
            "The 388th training loss is1111821707.0821705\n",
            "The 389th training loss is1111681378.203934\n",
            "The 390th training loss is1111543848.0363169\n",
            "The 391th training loss is1111409060.657075\n",
            "The 392th training loss is1111276961.2635484\n",
            "The 393th training loss is1111147496.1501985\n",
            "The 394th training loss is1111020612.6865997\n",
            "The 395th training loss is1110896259.2958717\n",
            "The 396th training loss is1110774385.4335456\n",
            "The 397th training loss is1110654941.566858\n",
            "The 398th training loss is1110537879.154456\n",
            "The 399th training loss is1110423150.6265142\n",
            "The 400th training loss is1110310709.3652508\n",
            "The 401th training loss is1110200509.6858342\n",
            "The 402th training loss is1110092506.8176737\n",
            "The 403th training loss is1109986656.8860865\n",
            "The 404th training loss is1109882916.8943336\n",
            "The 405th training loss is1109781244.7060158\n",
            "The 406th training loss is1109681599.0278237\n",
            "The 407th training loss is1109583939.3926349\n",
            "The 408th training loss is1109488226.1429503\n",
            "The 409th training loss is1109394420.4146636\n",
            "The 410th training loss is1109302484.1211557\n",
            "The 411th training loss is1109212379.9377098\n",
            "The 412th training loss is1109124071.286241\n",
            "The 413th training loss is1109037522.3203292\n",
            "The 414th training loss is1108952697.9105542\n",
            "The 415th training loss is1108869563.6301274\n",
            "The 416th training loss is1108788085.7408082\n",
            "The 417th training loss is1108708231.1791084\n",
            "The 418th training loss is1108629967.5427668\n",
            "The 419th training loss is1108553263.077502\n",
            "The 420th training loss is1108478086.6640277\n",
            "The 421th training loss is1108404407.8053296\n",
            "The 422th training loss is1108332196.6141975\n",
            "The 423th training loss is1108261423.8010075\n",
            "The 424th training loss is1108192060.6617517\n",
            "The 425th training loss is1108124079.0663037\n",
            "The 426th training loss is1108057451.446925\n",
            "The 427th training loss is1107992150.7869978\n",
            "The 428th training loss is1107928150.6099875\n",
            "The 429th training loss is1107865424.9686227\n",
            "The 430th training loss is1107803948.4342968\n",
            "The 431th training loss is1107743696.0866785\n",
            "The 432th training loss is1107684643.503534\n",
            "The 433th training loss is1107626766.7507508\n",
            "The 434th training loss is1107570042.3725636\n",
            "The 435th training loss is1107514447.3819747\n",
            "The 436th training loss is1107459959.251369\n",
            "The 437th training loss is1107406555.9033139\n",
            "The 438th training loss is1107354215.7015476\n",
            "The 439th training loss is1107302917.4421453\n",
            "The 440th training loss is1107252640.3448632\n",
            "The 441th training loss is1107203364.0446587\n",
            "The 442th training loss is1107155068.5833757\n",
            "The 443th training loss is1107107734.4016013\n",
            "The 444th training loss is1107061342.3306847\n",
            "The 445th training loss is1107015873.5849147\n",
            "The 446th training loss is1106971309.7538545\n",
            "The 447th training loss is1106927632.794832\n",
            "The 448th training loss is1106884825.0255787\n",
            "The 449th training loss is1106842869.117017\n",
            "The 450th training loss is1106801748.0861936\n",
            "The 451th training loss is1106761445.28935\n",
            "The 452th training loss is1106721944.4151382\n",
            "The 453th training loss is1106683229.477968\n",
            "The 454th training loss is1106645284.8114874\n",
            "The 455th training loss is1106608095.0621998\n",
            "The 456th training loss is1106571645.1831996\n",
            "The 457th training loss is1106535920.4280405\n",
            "The 458th training loss is1106500906.3447263\n",
            "The 459th training loss is1106466588.7698176\n",
            "The 460th training loss is1106432953.8226619\n",
            "The 461th training loss is1106399987.8997355\n",
            "The 462th training loss is1106367677.6691008\n",
            "The 463th training loss is1106336010.064974\n",
            "The 464th training loss is1106304972.2824025\n",
            "The 465th training loss is1106274551.7720475\n",
            "The 466th training loss is1106244736.2350724\n",
            "The 467th training loss is1106215513.6181326\n",
            "The 468th training loss is1106186872.1084673\n",
            "The 469th training loss is1106158800.1290882\n",
            "The 470th training loss is1106131286.3340638\n",
            "The 471th training loss is1106104319.6039016\n",
            "The 472th training loss is1106077889.041019\n",
            "The 473th training loss is1106051983.9653065\n",
            "The 474th training loss is1106026593.9097817\n",
            "The 475th training loss is1106001708.6163251\n",
            "The 476th training loss is1105977318.0315082\n",
            "The 477th training loss is1105953412.3024995\n",
            "The 478th training loss is1105929981.7730556\n",
            "The 479th training loss is1105907016.979591\n",
            "The 480th training loss is1105884508.6473267\n",
            "The 481th training loss is1105862447.6865191\n",
            "The 482th training loss is1105840825.1887586\n",
            "The 483th training loss is1105819632.423347\n",
            "The 484th training loss is1105798860.8337464\n",
            "The 485th training loss is1105778502.0340974\n",
            "The 486th training loss is1105758547.80581\n",
            "The 487th training loss is1105738990.09422\n",
            "The 488th training loss is1105719821.0053127\n",
            "The 489th training loss is1105701032.8025155\n",
            "The 490th training loss is1105682617.9035494\n",
            "The 491th training loss is1105664568.8773484\n",
            "The 492th training loss is1105646878.4410367\n",
            "The 493th training loss is1105629539.4569693\n",
            "The 494th training loss is1105612544.9298303\n",
            "The 495th training loss is1105595888.00379\n",
            "The 496th training loss is1105579561.9597178\n",
            "The 497th training loss is1105563560.2124522\n",
            "The 498th training loss is1105547876.308125\n",
            "The 499th training loss is1105532503.9215388\n",
            "The 500th training loss is1105517436.8535964\n",
            "The 501th training loss is1105502669.0287836\n",
            "The 502th training loss is1105488194.4926991\n",
            "The 503th training loss is1105474007.4096391\n",
            "The 504th training loss is1105460102.0602229\n",
            "The 505th training loss is1105446472.8390732\n",
            "The 506th training loss is1105433114.252539\n",
            "The 507th training loss is1105420020.916463\n",
            "The 508th training loss is1105407187.5539968\n",
            "The 509th training loss is1105394608.9934592\n",
            "The 510th training loss is1105382280.1662347\n",
            "The 511th training loss is1105370196.1047168\n",
            "The 512th training loss is1105358351.9402914\n",
            "The 513th training loss is1105346742.90136\n",
            "The 514th training loss is1105335364.3114042\n",
            "The 515th training loss is1105324211.5870872\n",
            "The 516th training loss is1105313280.236393\n",
            "The 517th training loss is1105302565.8568048\n",
            "The 518th training loss is1105292064.1335187\n",
            "The 519th training loss is1105281770.8376915\n",
            "The 520th training loss is1105271681.8247275\n",
            "The 521th training loss is1105261793.032595\n",
            "The 522th training loss is1105252100.4801788\n",
            "The 523th training loss is1105242600.2656682\n",
            "The 524th training loss is1105233288.5649705\n",
            "The 525th training loss is1105224161.630163\n",
            "The 526th training loss is1105215215.7879722\n",
            "The 527th training loss is1105206447.4382842\n",
            "The 528th training loss is1105197853.052685\n",
            "The 529th training loss is1105189429.1730313\n",
            "The 530th training loss is1105181172.4100463\n",
            "The 531th training loss is1105173079.4419472\n",
            "The 532th training loss is1105165147.013099\n",
            "The 533th training loss is1105157371.9326942\n",
            "The 534th training loss is1105149751.07346\n",
            "The 535th training loss is1105142281.370391\n",
            "The 536th training loss is1105134959.819507\n",
            "The 537th training loss is1105127783.476636\n",
            "The 538th training loss is1105120749.4562213\n",
            "The 539th training loss is1105113854.9301524\n",
            "The 540th training loss is1105107097.1266198\n",
            "The 541th training loss is1105100473.3289912\n",
            "The 542th training loss is1105093980.8747118\n",
            "The 543th training loss is1105087617.1542263\n",
            "The 544th training loss is1105081379.6099215\n",
            "The 545th training loss is1105075265.7350903\n",
            "The 546th training loss is1105069273.0729182\n",
            "The 547th training loss is1105063399.2154863\n",
            "The 548th training loss is1105057641.8027978\n",
            "The 549th training loss is1105051998.5218227\n",
            "The 550th training loss is1105046467.1055615\n",
            "The 551th training loss is1105041045.3321261\n",
            "The 552th training loss is1105035731.0238426\n",
            "The 553th training loss is1105030522.0463684\n",
            "The 554th training loss is1105025416.3078291\n",
            "The 555th training loss is1105020411.7579718\n",
            "The 556th training loss is1105015506.3873358\n",
            "The 557th training loss is1105010698.226439\n",
            "The 558th training loss is1105005985.344982\n",
            "The 559th training loss is1105001365.8510666\n",
            "The 560th training loss is1104996837.8904297\n",
            "The 561th training loss is1104992399.6456954\n",
            "The 562th training loss is1104988049.3356376\n",
            "The 563th training loss is1104983785.2144613\n",
            "The 564th training loss is1104979605.571096\n",
            "The 565th training loss is1104975508.728503\n",
            "The 566th training loss is1104971493.0429997\n",
            "The 567th training loss is1104967556.9035923\n",
            "The 568th training loss is1104963698.7313254\n",
            "The 569th training loss is1104959916.9786446\n",
            "The 570th training loss is1104956210.1287706\n",
            "The 571th training loss is1104952576.6950846\n",
            "The 572th training loss is1104949015.2205305\n",
            "The 573th training loss is1104945524.277022\n",
            "The 574th training loss is1104942102.4648693\n",
            "The 575th training loss is1104938748.412212\n",
            "The 576th training loss is1104935460.7744634\n",
            "The 577th training loss is1104932238.2337701\n",
            "The 578th training loss is1104929079.4984775\n",
            "The 579th training loss is1104925983.3026085\n",
            "The 580th training loss is1104922948.4053524\n",
            "The 581th training loss is1104919973.5905633\n",
            "The 582th training loss is1104917057.6662693\n",
            "The 583th training loss is1104914199.464192\n",
            "The 584th training loss is1104911397.8392723\n",
            "The 585th training loss is1104908651.6692102\n",
            "The 586th training loss is1104905959.854011\n",
            "The 587th training loss is1104903321.3155415\n",
            "The 588th training loss is1104900734.9970937\n",
            "The 589th training loss is1104898199.862959\n",
            "The 590th training loss is1104895714.8980112\n",
            "The 591th training loss is1104893279.1072938\n",
            "The 592th training loss is1104890891.5156224\n",
            "The 593th training loss is1104888551.167188\n",
            "The 594th training loss is1104886257.1251721\n",
            "The 595th training loss is1104884008.47137\n",
            "The 596th training loss is1104881804.3058183\n",
            "The 597th training loss is1104879643.7464333\n",
            "The 598th training loss is1104877525.9286554\n",
            "The 599th training loss is1104875450.0050986\n",
            "The 600th training loss is1104873415.14521\n",
            "The 601th training loss is1104871420.534935\n",
            "The 602th training loss is1104869465.3763888\n",
            "The 603th training loss is1104867548.8875337\n",
            "The 604th training loss is1104865670.3018649\n",
            "The 605th training loss is1104863828.868101\n",
            "The 606th training loss is1104862023.8498812\n",
            "The 607th training loss is1104860254.5254679\n",
            "The 608th training loss is1104858520.1874568\n",
            "The 609th training loss is1104856820.1424916\n",
            "The 610th training loss is1104855153.7109828\n",
            "The 611th training loss is1104853520.2268362\n",
            "The 612th training loss is1104851919.037183\n",
            "The 613th training loss is1104850349.502117\n",
            "The 614th training loss is1104848810.9944375\n",
            "The 615th training loss is1104847302.8993952\n",
            "The 616th training loss is1104845824.6144454\n",
            "The 617th training loss is1104844375.5490055\n",
            "The 618th training loss is1104842955.1242168\n",
            "The 619th training loss is1104841562.7727106\n",
            "The 620th training loss is1104840197.9383807\n",
            "The 621th training loss is1104838860.0761602\n",
            "The 622th training loss is1104837548.651799\n",
            "The 623th training loss is1104836263.1416516\n",
            "The 624th training loss is1104835003.0324657\n",
            "The 625th training loss is1104833767.8211741\n",
            "The 626th training loss is1104832557.0146933\n",
            "The 627th training loss is1104831370.129726\n",
            "The 628th training loss is1104830206.692564\n",
            "The 629th training loss is1104829066.2388992\n",
            "The 630th training loss is1104827948.3136375\n",
            "The 631th training loss is1104826852.4707136\n",
            "The 632th training loss is1104825778.2729127\n",
            "The 633th training loss is1104824725.2916954\n",
            "The 634th training loss is1104823693.1070225\n",
            "The 635th training loss is1104822681.3071895\n",
            "The 636th training loss is1104821689.4886584\n",
            "The 637th training loss is1104820717.2558959\n",
            "The 638th training loss is1104819764.221216\n",
            "The 639th training loss is1104818830.004621\n",
            "The 640th training loss is1104817914.2336519\n",
            "The 641th training loss is1104817016.5432365\n",
            "The 642th training loss is1104816136.575543\n",
            "The 643th training loss is1104815273.9798362\n",
            "The 644th training loss is1104814428.4123378\n",
            "The 645th training loss is1104813599.5360856\n",
            "The 646th training loss is1104812787.0208004\n",
            "The 647th training loss is1104811990.5427513\n",
            "The 648th training loss is1104811209.7846282\n",
            "The 649th training loss is1104810444.4354112\n",
            "The 650th training loss is1104809694.1902475\n",
            "The 651th training loss is1104808958.7503288\n",
            "The 652th training loss is1104808237.8227704\n",
            "The 653th training loss is1104807531.1204946\n",
            "The 654th training loss is1104806838.3621144\n",
            "The 655th training loss is1104806159.271821\n",
            "The 656th training loss is1104805493.5792723\n",
            "The 657th training loss is1104804841.0194862\n",
            "The 658th training loss is1104804201.332731\n",
            "The 659th training loss is1104803574.2644253\n",
            "The 660th training loss is1104802959.5650308\n",
            "The 661th training loss is1104802356.989957\n",
            "The 662th training loss is1104801766.2994597\n",
            "The 663th training loss is1104801187.2585452\n",
            "The 664th training loss is1104800619.6368775\n",
            "The 665th training loss is1104800063.208685\n",
            "The 666th training loss is1104799517.7526681\n",
            "The 667th training loss is1104798983.0519147\n",
            "The 668th training loss is1104798458.8938072\n",
            "The 669th training loss is1104797945.0699437\n",
            "The 670th training loss is1104797441.3760488\n",
            "The 671th training loss is1104796947.6118953\n",
            "The 672th training loss is1104796463.5812218\n",
            "The 673th training loss is1104795989.0916553\n",
            "The 674th training loss is1104795523.9546328\n",
            "The 675th training loss is1104795067.9853265\n",
            "The 676th training loss is1104794621.0025697\n",
            "The 677th training loss is1104794182.828784\n",
            "The 678th training loss is1104793753.2899077\n",
            "The 679th training loss is1104793332.215326\n",
            "The 680th training loss is1104792919.4378042\n",
            "The 681th training loss is1104792514.793418\n",
            "The 682th training loss is1104792118.1214895\n",
            "The 683th training loss is1104791729.2645226\n",
            "The 684th training loss is1104791348.0681398\n",
            "The 685th training loss is1104790974.3810205\n",
            "The 686th training loss is1104790608.054839\n",
            "The 687th training loss is1104790248.944207\n",
            "The 688th training loss is1104789896.9066145\n",
            "The 689th training loss is1104789551.8023727\n",
            "The 690th training loss is1104789213.4945579\n",
            "The 691th training loss is1104788881.8489563\n",
            "The 692th training loss is1104788556.7340117\n",
            "The 693th training loss is1104788238.0207708\n",
            "The 694th training loss is1104787925.5828333\n",
            "The 695th training loss is1104787619.2963006\n",
            "The 696th training loss is1104787319.0397255\n",
            "The 697th training loss is1104787024.6940649\n",
            "The 698th training loss is1104786736.1426313\n",
            "The 699th training loss is1104786453.2710454\n",
            "The 700th training loss is1104786175.9671926\n",
            "The 701th training loss is1104785904.1211755\n",
            "The 702th training loss is1104785637.6252718\n",
            "The 703th training loss is1104785376.3738897\n",
            "The 704th training loss is1104785120.2635276\n",
            "The 705th training loss is1104784869.1927307\n",
            "The 706th training loss is1104784623.0620506\n",
            "The 707th training loss is1104784381.7740078\n",
            "The 708th training loss is1104784145.2330496\n",
            "The 709th training loss is1104783913.3455138\n",
            "The 710th training loss is1104783686.0195909\n",
            "The 711th training loss is1104783463.1652865\n",
            "The 712th training loss is1104783244.6943877\n",
            "The 713th training loss is1104783030.520424\n",
            "The 714th training loss is1104782820.5586364\n",
            "The 715th training loss is1104782614.7259412\n",
            "The 716th training loss is1104782412.9408984\n",
            "The 717th training loss is1104782215.123677\n",
            "The 718th training loss is1104782021.1960256\n",
            "The 719th training loss is1104781831.0812402\n",
            "The 720th training loss is1104781644.7041316\n",
            "The 721th training loss is1104781461.9909985\n",
            "The 722th training loss is1104781282.869596\n",
            "The 723th training loss is1104781107.2691078\n",
            "The 724th training loss is1104780935.1201165\n",
            "The 725th training loss is1104780766.3545778\n",
            "The 726th training loss is1104780600.9057918\n",
            "The 727th training loss is1104780438.7083766\n",
            "The 728th training loss is1104780279.6982431\n",
            "The 729th training loss is1104780123.8125687\n",
            "The 730th training loss is1104779970.9897711\n",
            "The 731th training loss is1104779821.169487\n",
            "The 732th training loss is1104779674.2925441\n",
            "The 733th training loss is1104779530.3009398\n",
            "The 734th training loss is1104779389.1378174\n",
            "The 735th training loss is1104779250.7474444\n",
            "The 736th training loss is1104779115.075188\n",
            "The 737th training loss is1104778982.0674958\n",
            "The 738th training loss is1104778851.6718726\n",
            "The 739th training loss is1104778723.8368607\n",
            "The 740th training loss is1104778598.5120177\n",
            "The 741th training loss is1104778475.6478994\n",
            "The 742th training loss is1104778355.1960368\n",
            "The 743th training loss is1104778237.108918\n",
            "The 744th training loss is1104778121.33997\n",
            "The 745th training loss is1104778007.8435402\n",
            "The 746th training loss is1104777896.5748758\n",
            "The 747th training loss is1104777787.4901085\n",
            "The 748th training loss is1104777680.5462372\n",
            "The 749th training loss is1104777575.701108\n",
            "The 750th training loss is1104777472.9134004\n",
            "The 751th training loss is1104777372.1426098\n",
            "The 752th training loss is1104777273.3490305\n",
            "The 753th training loss is1104777176.493741\n",
            "The 754th training loss is1104777081.538588\n",
            "The 755th training loss is1104776988.4461713\n",
            "The 756th training loss is1104776897.1798291\n",
            "The 757th training loss is1104776807.7036233\n",
            "The 758th training loss is1104776719.9823244\n",
            "The 759th training loss is1104776633.9813986\n",
            "The 760th training loss is1104776549.6669931\n",
            "The 761th training loss is1104776467.0059247\n",
            "The 762th training loss is1104776385.9656627\n",
            "The 763th training loss is1104776306.5143197\n",
            "The 764th training loss is1104776228.6206372\n",
            "The 765th training loss is1104776152.2539735\n",
            "The 766th training loss is1104776077.384291\n",
            "The 767th training loss is1104776003.9821448\n",
            "The 768th training loss is1104775932.0186708\n",
            "The 769th training loss is1104775861.465575\n",
            "The 770th training loss is1104775792.2951205\n",
            "The 771th training loss is1104775724.4801173\n",
            "The 772th training loss is1104775657.9939127\n",
            "The 773th training loss is1104775592.8103793\n",
            "The 774th training loss is1104775528.9039042\n",
            "The 775th training loss is1104775466.24938\n",
            "The 776th training loss is1104775404.822195\n",
            "The 777th training loss is1104775344.5982218\n",
            "The 778th training loss is1104775285.5538094\n",
            "The 779th training loss is1104775227.6657727\n",
            "The 780th training loss is1104775170.9113836\n",
            "The 781th training loss is1104775115.2683628\n",
            "The 782th training loss is1104775060.714869\n",
            "The 783th training loss is1104775007.2294927\n",
            "The 784th training loss is1104774954.791245\n",
            "The 785th training loss is1104774903.3795516\n",
            "The 786th training loss is1104774852.9742444\n",
            "The 787th training loss is1104774803.5555503\n",
            "The 788th training loss is1104774755.1040888\n",
            "The 789th training loss is1104774707.600859\n",
            "The 790th training loss is1104774661.027236\n",
            "The 791th training loss is1104774615.3649602\n",
            "The 792th training loss is1104774570.5961332\n",
            "The 793th training loss is1104774526.7032092\n",
            "The 794th training loss is1104774483.668987\n",
            "The 795th training loss is1104774441.476605\n",
            "The 796th training loss is1104774400.1095335\n",
            "The 797th training loss is1104774359.5515685\n",
            "The 798th training loss is1104774319.7868252\n",
            "The 799th training loss is1104774280.799731\n",
            "The 800th training loss is1104774242.5750215\n",
            "The 801th training loss is1104774205.0977309\n",
            "The 802th training loss is1104774168.3531897\n",
            "The 803th training loss is1104774132.3270164\n",
            "The 804th training loss is1104774097.0051131\n",
            "The 805th training loss is1104774062.3736594\n",
            "The 806th training loss is1104774028.4191067\n",
            "The 807th training loss is1104773995.128173\n",
            "The 808th training loss is1104773962.4878387\n",
            "The 809th training loss is1104773930.4853392\n",
            "The 810th training loss is1104773899.1081624\n",
            "The 811th training loss is1104773868.3440413\n",
            "The 812th training loss is1104773838.1809514\n",
            "The 813th training loss is1104773808.6071033\n",
            "The 814th training loss is1104773779.610942\n",
            "The 815th training loss is1104773751.1811368\n",
            "The 816th training loss is1104773723.306583\n",
            "The 817th training loss is1104773695.9763925\n",
            "The 818th training loss is1104773669.179892\n",
            "The 819th training loss is1104773642.9066184\n",
            "The 820th training loss is1104773617.146313\n",
            "The 821th training loss is1104773591.8889213\n",
            "The 822th training loss is1104773567.1245842\n",
            "The 823th training loss is1104773542.843639\n",
            "The 824th training loss is1104773519.03661\n",
            "The 825th training loss is1104773495.69421\n",
            "The 826th training loss is1104773472.807335\n",
            "The 827th training loss is1104773450.3670576\n",
            "The 828th training loss is1104773428.3646283\n",
            "The 829th training loss is1104773406.7914674\n",
            "The 830th training loss is1104773385.639166\n",
            "The 831th training loss is1104773364.8994796\n",
            "The 832th training loss is1104773344.5643256\n",
            "The 833th training loss is1104773324.6257799\n",
            "The 834th training loss is1104773305.0760756\n",
            "The 835th training loss is1104773285.907597\n",
            "The 836th training loss is1104773267.1128788\n",
            "The 837th training loss is1104773248.6846025\n",
            "The 838th training loss is1104773230.6155927\n",
            "The 839th training loss is1104773212.8988156\n",
            "The 840th training loss is1104773195.5273755\n",
            "The 841th training loss is1104773178.4945126\n",
            "The 842th training loss is1104773161.7935987\n",
            "The 843th training loss is1104773145.4181368\n",
            "The 844th training loss is1104773129.3617575\n",
            "The 845th training loss is1104773113.6182165\n",
            "The 846th training loss is1104773098.1813915\n",
            "The 847th training loss is1104773083.045281\n",
            "The 848th training loss is1104773068.2040012\n",
            "The 849th training loss is1104773053.6517844\n",
            "The 850th training loss is1104773039.3829749\n",
            "The 851th training loss is1104773025.3920298\n",
            "The 852th training loss is1104773011.6735127\n",
            "The 853th training loss is1104772998.2220964\n",
            "The 854th training loss is1104772985.0325572\n",
            "The 855th training loss is1104772972.0997734\n",
            "The 856th training loss is1104772959.4187245\n",
            "The 857th training loss is1104772946.9844882\n",
            "The 858th training loss is1104772934.7922397\n",
            "The 859th training loss is1104772922.8372471\n",
            "The 860th training loss is1104772911.1148736\n",
            "The 861th training loss is1104772899.6205711\n",
            "The 862th training loss is1104772888.349883\n",
            "The 863th training loss is1104772877.298438\n",
            "The 864th training loss is1104772866.4619527\n",
            "The 865th training loss is1104772855.8362257\n",
            "The 866th training loss is1104772845.4171388\n",
            "The 867th training loss is1104772835.200656\n",
            "The 868th training loss is1104772825.182818\n",
            "The 869th training loss is1104772815.359744\n",
            "The 870th training loss is1104772805.7276313\n",
            "The 871th training loss is1104772796.2827485\n",
            "The 872th training loss is1104772787.0214396\n",
            "The 873th training loss is1104772777.94012\n",
            "The 874th training loss is1104772769.0352743\n",
            "The 875th training loss is1104772760.3034575\n",
            "The 876th training loss is1104772751.7412908\n",
            "The 877th training loss is1104772743.3454628\n",
            "The 878th training loss is1104772735.112726\n",
            "The 879th training loss is1104772727.0398972\n",
            "The 880th training loss is1104772719.123855\n",
            "The 881th training loss is1104772711.3615403\n",
            "The 882th training loss is1104772703.7499526\n",
            "The 883th training loss is1104772696.286151\n",
            "The 884th training loss is1104772688.9672518\n",
            "The 885th training loss is1104772681.7904282\n",
            "The 886th training loss is1104772674.7529085\n",
            "The 887th training loss is1104772667.8519757\n",
            "The 888th training loss is1104772661.0849652\n",
            "The 889th training loss is1104772654.4492657\n",
            "The 890th training loss is1104772647.9423156\n",
            "The 891th training loss is1104772641.561605\n",
            "The 892th training loss is1104772635.304672\n",
            "The 893th training loss is1104772629.1691036\n",
            "The 894th training loss is1104772623.1525335\n",
            "The 895th training loss is1104772617.2526426\n",
            "The 896th training loss is1104772611.4671564\n",
            "The 897th training loss is1104772605.7938445\n",
            "The 898th training loss is1104772600.2305217\n",
            "The 899th training loss is1104772594.7750444\n",
            "The 900th training loss is1104772589.4253109\n",
            "The 901th training loss is1104772584.1792612\n",
            "The 902th training loss is1104772579.0348754\n",
            "The 903th training loss is1104772573.9901729\n",
            "The 904th training loss is1104772569.0432122\n",
            "The 905th training loss is1104772564.1920898\n",
            "The 906th training loss is1104772559.4349391\n",
            "The 907th training loss is1104772554.7699296\n",
            "The 908th training loss is1104772550.1952682\n",
            "The 909th training loss is1104772545.7091951\n",
            "The 910th training loss is1104772541.309986\n",
            "The 911th training loss is1104772536.99595\n",
            "The 912th training loss is1104772532.765429\n",
            "The 913th training loss is1104772528.616798\n",
            "The 914th training loss is1104772524.5484626\n",
            "The 915th training loss is1104772520.5588608\n",
            "The 916th training loss is1104772516.6464608\n",
            "The 917th training loss is1104772512.80976\n",
            "The 918th training loss is1104772509.0472865\n",
            "The 919th training loss is1104772505.3575957\n",
            "The 920th training loss is1104772501.7392724\n",
            "The 921th training loss is1104772498.1909285\n",
            "The 922th training loss is1104772494.7112029\n",
            "The 923th training loss is1104772491.2987614\n",
            "The 924th training loss is1104772487.952296\n",
            "The 925th training loss is1104772484.670524\n",
            "The 926th training loss is1104772481.4521873\n",
            "The 927th training loss is1104772478.2960534\n",
            "The 928th training loss is1104772475.2009132\n",
            "The 929th training loss is1104772472.165581\n",
            "The 930th training loss is1104772469.1888947\n",
            "The 931th training loss is1104772466.2697153\n",
            "The 932th training loss is1104772463.406925\n",
            "The 933th training loss is1104772460.5994284\n",
            "The 934th training loss is1104772457.8461514\n",
            "The 935th training loss is1104772455.146041\n",
            "The 936th training loss is1104772452.4980645\n",
            "The 937th training loss is1104772449.9012098\n",
            "The 938th training loss is1104772447.3544838\n",
            "The 939th training loss is1104772444.8569136\n",
            "The 940th training loss is1104772442.4075444\n",
            "The 941th training loss is1104772440.005441\n",
            "The 942th training loss is1104772437.6496859\n",
            "The 943th training loss is1104772435.3393793\n",
            "The 944th training loss is1104772433.0736396\n",
            "The 945th training loss is1104772430.8516016\n",
            "The 946th training loss is1104772428.6724176\n",
            "The 947th training loss is1104772426.5352561\n",
            "The 948th training loss is1104772424.4393022\n",
            "The 949th training loss is1104772422.3837564\n",
            "The 950th training loss is1104772420.367835\n",
            "The 951th training loss is1104772418.3907695\n",
            "The 952th training loss is1104772416.4518068\n",
            "The 953th training loss is1104772414.5502076\n",
            "The 954th training loss is1104772412.685248\n",
            "The 955th training loss is1104772410.8562174\n",
            "The 956th training loss is1104772409.0624194\n",
            "The 957th training loss is1104772407.3031716\n",
            "The 958th training loss is1104772405.5778039\n",
            "The 959th training loss is1104772403.8856602\n",
            "The 960th training loss is1104772402.2260969\n",
            "The 961th training loss is1104772400.5984824\n",
            "The 962th training loss is1104772399.0021977\n",
            "The 963th training loss is1104772397.4366362\n",
            "The 964th training loss is1104772395.9012034\n",
            "The 965th training loss is1104772394.3953152\n",
            "The 966th training loss is1104772392.9183998\n",
            "The 967th training loss is1104772391.4698958\n",
            "The 968th training loss is1104772390.049254\n",
            "The 969th training loss is1104772388.6559346\n",
            "The 970th training loss is1104772387.2894087\n",
            "The 971th training loss is1104772385.949158\n",
            "The 972th training loss is1104772384.634674\n",
            "The 973th training loss is1104772383.345458\n",
            "The 974th training loss is1104772382.0810213\n",
            "The 975th training loss is1104772380.8408847\n",
            "The 976th training loss is1104772379.6245778\n",
            "The 977th training loss is1104772378.43164\n",
            "The 978th training loss is1104772377.2616196\n",
            "The 979th training loss is1104772376.1140728\n",
            "The 980th training loss is1104772374.9885654\n",
            "The 981th training loss is1104772373.8846717\n",
            "The 982th training loss is1104772372.8019738\n",
            "The 983th training loss is1104772371.7400618\n",
            "The 984th training loss is1104772370.6985338\n",
            "The 985th training loss is1104772369.6769965\n",
            "The 986th training loss is1104772368.6750631\n",
            "The 987th training loss is1104772367.6923552\n",
            "The 988th training loss is1104772366.7285018\n",
            "The 989th training loss is1104772365.7831378\n",
            "The 990th training loss is1104772364.855907\n",
            "The 991th training loss is1104772363.9464583\n",
            "The 992th training loss is1104772363.0544493\n",
            "The 993th training loss is1104772362.179543\n",
            "The 994th training loss is1104772361.321409\n",
            "The 995th training loss is1104772360.479724\n",
            "The 996th training loss is1104772359.65417\n",
            "The 997th training loss is1104772358.8444362\n",
            "The 998th training loss is1104772358.0502172\n",
            "The 999th training loss is1104772357.2712138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 7] Learning curve plot"
      ],
      "metadata": {
        "id": "FmGP0V-M1No9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(slr.loss)\n",
        "plt.plot(slr.val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "dhHVBvVE09ts",
        "outputId": "ac65b502-eca9-4c98-8a85-69eb53605952"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f196b507dc0>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}