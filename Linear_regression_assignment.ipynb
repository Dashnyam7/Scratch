{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN3UpxBCK4KWEfmI9iHf1kg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dashnyam7/Scratch/blob/main/Linear_regression_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1] Hypothetical function"
      ],
      "metadata": {
        "id": "BzOqcadknccr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6LavBL5knWzS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 2] The gradient descent method"
      ],
      "metadata": {
        "id": "pU8eObB3pYpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, error):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - error[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return\n"
      ],
      "metadata": {
        "id": "TnAWsZgrpiuW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 3] Presumption"
      ],
      "metadata": {
        "id": "vUWTFbnGpcvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y"
      ],
      "metadata": {
        "id": "rXT9Pkh8rDTN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 4]  Mean squared error"
      ],
      "metadata": {
        "id": "RTfkShIXuHYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / y.shape[0]\n",
        "        return mse"
      ],
      "metadata": {
        "id": "scmjINLDuIyH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 5] Objective function"
      ],
      "metadata": {
        "id": "TsZw-XbXvMTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / y.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self,y_pred, y):\n",
        "        loss = self.MSE(pred, y)/2\n",
        "        return loss"
      ],
      "metadata": {
        "id": "xzF9PGULvQ-y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 6] Learning and Estimation"
      ],
      "metadata": {
        "id": "TapFPhUgvnxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        #self.loss = np.zeros(self.iter)\n",
        "        #self.val_loss = np.zeros(self.iter)\n",
        "        self.theta = np.array([])\n",
        "        self.loss = np.array([])\n",
        "        self.val_loss = np.array([])\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        y_pred = X @ self.theta\n",
        "        return y_pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      y_pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (y_pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)   \n",
        "\n",
        "    def MSE(self, y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / y.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self, y_pred, y):\n",
        "        loss = self.MSE(y_pred, y)/2\n",
        "        return loss \n",
        "        \n",
        "    def fit(self, X, y, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"        \n",
        "        if self.no_bias == True:\n",
        "            no_bias = np.ones((X.shape[0], 1))\n",
        "            X = np.hstack((no_bias, X))\n",
        "            no_bias = np.ones((X_val.shape[0], 1))\n",
        "            X_val = np.hstack((no_bias, X_val))\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "        for i in range(self.iter):\n",
        "            pred = self._linear_hypothesis(X)\n",
        "            pred_val = self._linear_hypothesis(X_val)\n",
        "            self._gradient_descent(X, y)\n",
        "            loss = self._loss_func(pred, y)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            loss_val = self._loss_func(pred_val, y_val)\n",
        "            self.val_loss = np.append(self.val_loss, loss_val)\n",
        "            if self.verbose:\n",
        "                #Output learning process when verbose is set to True\n",
        "                print('The {}th training loss is{}'.format(i,loss))    \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "Y1qcCXdXv7fw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "UjDh3aZNwi6m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
        "slr.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEfB1FOewmpd",
        "outputId": "dcd02685-030b-4a06-a538-8bd483b5a5ce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 0th training loss is19391233628.974743\n",
            "The 1th training loss is19019003460.85563\n",
            "The 2th training loss is18654364269.041958\n",
            "The 3th training loss is18297160542.06334\n",
            "The 4th training loss is17947239970.801975\n",
            "The 5th training loss is17604453382.164806\n",
            "The 6th training loss is17268654674.13831\n",
            "The 7th training loss is16939700752.197012\n",
            "The 8th training loss is16617451467.03724\n",
            "The 9th training loss is16301769553.608292\n",
            "The 10th training loss is15992520571.413862\n",
            "The 11th training loss is15689572846.056995\n",
            "The 12th training loss is15392797412.002535\n",
            "The 13th training loss is15102067956.531504\n",
            "The 14th training loss is14817260764.862429\n",
            "The 15th training loss is14538254666.415155\n",
            "The 16th training loss is14264930982.193169\n",
            "The 17th training loss is13997173473.261011\n",
            "The 18th training loss is13734868290.293821\n",
            "The 19th training loss is13477903924.176544\n",
            "The 20th training loss is13226171157.630812\n",
            "The 21th training loss is12979563017.847988\n",
            "The 22th training loss is12737974730.107252\n",
            "The 23th training loss is12501303672.358189\n",
            "The 24th training loss is12269449330.747572\n",
            "The 25th training loss is12042313256.07069\n",
            "The 26th training loss is11819799021.127773\n",
            "The 27th training loss is11601812178.96664\n",
            "The 28th training loss is11388260221.992996\n",
            "The 29th training loss is11179052541.930243\n",
            "The 30th training loss is10974100390.611\n",
            "The 31th training loss is10773316841.582994\n",
            "The 32th training loss is10576616752.51225\n",
            "The 33th training loss is10383916728.366934\n",
            "The 34th training loss is10195135085.365503\n",
            "The 35th training loss is10010191815.67323\n",
            "The 36th training loss is9829008552.831427\n",
            "The 37th training loss is9651508537.904078\n",
            "The 38th training loss is9477616586.32689\n",
            "The 39th training loss is9307259055.444088\n",
            "The 40th training loss is9140363812.718588\n",
            "The 41th training loss is8976860204.6015\n",
            "The 42th training loss is8816679026.047194\n",
            "The 43th training loss is8659752490.66043\n",
            "The 44th training loss is8506014201.462396\n",
            "The 45th training loss is8355399122.262707\n",
            "The 46th training loss is8207843549.624752\n",
            "The 47th training loss is8063285085.411981\n",
            "The 48th training loss is7921662609.903026\n",
            "The 49th training loss is7782916255.46382\n",
            "The 50th training loss is7646987380.765048\n",
            "The 51th training loss is7513818545.533605\n",
            "The 52th training loss is7383353485.826894\n",
            "The 53th training loss is7255537089.819122\n",
            "The 54th training loss is7130315374.088843\n",
            "The 55th training loss is7007635460.39738\n",
            "The 56th training loss is6887445552.947856\n",
            "The 57th training loss is6769694916.114844\n",
            "The 58th training loss is6654333852.634817\n",
            "The 59th training loss is6541313682.247814\n",
            "The 60th training loss is6430586720.780934\n",
            "The 61th training loss is6322106259.664444\n",
            "The 62th training loss is6215826545.871502\n",
            "The 63th training loss is6111702762.272692\n",
            "The 64th training loss is6009691008.396725\n",
            "The 65th training loss is5909748281.588851\n",
            "The 66th training loss is5811832458.558731\n",
            "The 67th training loss is5715902277.309654\n",
            "The 68th training loss is5621917319.441158\n",
            "The 69th training loss is5529837992.817322\n",
            "The 70th training loss is5439625514.593091\n",
            "The 71th training loss is5351241894.591227\n",
            "The 72th training loss is5264649919.022556\n",
            "The 73th training loss is5179813134.542432\n",
            "The 74th training loss is5096695832.636378\n",
            "The 75th training loss is5015263034.328101\n",
            "The 76th training loss is4935480475.20317\n",
            "The 77th training loss is4857314590.741808\n",
            "The 78th training loss is4780732501.954374\n",
            "The 79th training loss is4705702001.313248\n",
            "The 80th training loss is4632191538.974977\n",
            "The 81th training loss is4560170209.286645\n",
            "The 82th training loss is4489607737.570576\n",
            "The 83th training loss is4420474467.181594\n",
            "The 84th training loss is4352741346.831171\n",
            "The 85th training loss is4286379918.172955\n",
            "The 86th training loss is4221362303.6442304\n",
            "The 87th training loss is4157661194.5580077\n",
            "The 88th training loss is4095249839.440565\n",
            "The 89th training loss is4034102032.6093197\n",
            "The 90th training loss is3974192102.9860787\n",
            "The 91th training loss is3915494903.140769\n",
            "The 92th training loss is3857985798.560881\n",
            "The 93th training loss is3801640657.1419477\n",
            "The 94th training loss is3746435838.894465\n",
            "The 95th training loss is3692348185.8627996\n",
            "The 96th training loss is3639355012.251663\n",
            "The 97th training loss is3587434094.7558684\n",
            "The 98th training loss is3536563663.089158\n",
            "The 99th training loss is3486722390.7079825\n",
            "The 100th training loss is3437889385.7261853\n",
            "The 101th training loss is3390044182.01666\n",
            "The 102th training loss is3343166730.496086\n",
            "The 103th training loss is3297237390.5889797\n",
            "The 104th training loss is3252236921.867328\n",
            "The 105th training loss is3208146475.8621926\n",
            "The 106th training loss is3164947588.0437055\n",
            "The 107th training loss is3122622169.965998\n",
            "The 108th training loss is3081152501.573629\n",
            "The 109th training loss is3040521223.666191\n",
            "The 110th training loss is3000711330.5178123\n",
            "The 111th training loss is2961706162.6483574\n",
            "The 112th training loss is2923489399.7431927\n",
            "The 113th training loss is2886045053.718446\n",
            "The 114th training loss is2849357461.9287467\n",
            "The 115th training loss is2813411280.514514\n",
            "The 116th training loss is2778191477.8859043\n",
            "The 117th training loss is2743683328.340594\n",
            "The 118th training loss is2709872405.812634\n",
            "The 119th training loss is2676744577.7496758\n",
            "The 120th training loss is2644285999.115909\n",
            "The 121th training loss is2612483106.518122\n",
            "The 122th training loss is2581322612.4523435\n",
            "The 123th training loss is2550791499.668567\n",
            "The 124th training loss is2520877015.6511407\n",
            "The 125th training loss is2491566667.2124133\n",
            "The 126th training loss is2462848215.19732\n",
            "The 127th training loss is2434709669.296604\n",
            "The 128th training loss is2407139282.9664416\n",
            "The 129th training loss is2380125548.4522743\n",
            "The 130th training loss is2353657191.9146996\n",
            "The 131th training loss is2327723168.655316\n",
            "The 132th training loss is2302312658.440454\n",
            "The 133th training loss is2277415060.920796\n",
            "The 134th training loss is2253019991.144886\n",
            "The 135th training loss is2229117275.1646047\n",
            "The 136th training loss is2205696945.730714\n",
            "The 137th training loss is2182749238.0766196\n",
            "The 138th training loss is2160264585.788525\n",
            "The 139th training loss is2138233616.7602017\n",
            "The 140th training loss is2116647149.230641\n",
            "The 141th training loss is2095496187.9028604\n",
            "The 142th training loss is2074771920.142223\n",
            "The 143th training loss is2054465712.2525995\n",
            "The 144th training loss is2034569105.8288023\n",
            "The 145th training loss is2015073814.1836948\n",
            "The 146th training loss is1995971718.848459\n",
            "The 147th training loss is1977254866.1445034\n",
            "The 148th training loss is1958915463.8255448\n",
            "The 149th training loss is1940945877.7884145\n",
            "The 150th training loss is1923338628.851183\n",
            "The 151th training loss is1906086389.597208\n",
            "The 152th training loss is1889181981.2837584\n",
            "The 153th training loss is1872618370.8138852\n",
            "The 154th training loss is1856388667.7702322\n",
            "The 155th training loss is1840486121.509525\n",
            "The 156th training loss is1824904118.3164806\n",
            "The 157th training loss is1809636178.6159208\n",
            "The 158th training loss is1794675954.2418888\n",
            "The 159th training loss is1780017225.7626116\n",
            "The 160th training loss is1765653899.8601375\n",
            "The 161th training loss is1751580006.7635489\n",
            "The 162th training loss is1737789697.7346346\n",
            "The 163th training loss is1724277242.6049504\n",
            "The 164th training loss is1711037027.363211\n",
            "The 165th training loss is1698063551.7919815\n",
            "The 166th training loss is1685351427.1526508\n",
            "The 167th training loss is1672895373.917705\n",
            "The 168th training loss is1660690219.54932\n",
            "The 169th training loss is1648730896.3233325\n",
            "The 170th training loss is1637012439.1976469\n",
            "The 171th training loss is1625529983.7241745\n",
            "The 172th training loss is1614278764.0034125\n",
            "The 173th training loss is1603254110.68078\n",
            "The 174th training loss is1592451448.983864\n",
            "The 175th training loss is1581866296.7997286\n",
            "The 176th training loss is1571494262.7914684\n",
            "The 177th training loss is1561331044.553205\n",
            "The 178th training loss is1551372426.8027334\n",
            "The 179th training loss is1541614279.6110485\n",
            "The 180th training loss is1532052556.6679943\n",
            "The 181th training loss is1522683293.5832968\n",
            "The 182th training loss is1513502606.2222588\n",
            "The 183th training loss is1504506689.075394\n",
            "The 184th training loss is1495691813.6613216\n",
            "The 185th training loss is1487054326.96223\n",
            "The 186th training loss is1478590649.8912418\n",
            "The 187th training loss is1470297275.7910283\n",
            "The 188th training loss is1462170768.9630406\n",
            "The 189th training loss is1454207763.2267141\n",
            "The 190th training loss is1446404960.5080533\n",
            "The 191th training loss is1438759129.4569733\n",
            "The 192th training loss is1431267104.09283\n",
            "The 193th training loss is1423925782.4775457\n",
            "The 194th training loss is1416732125.4157758\n",
            "The 195th training loss is1409683155.181557\n",
            "The 196th training loss is1402775954.2708986\n",
            "The 197th training loss is1396007664.179782\n",
            "The 198th training loss is1389375484.207057\n",
            "The 199th training loss is1382876670.281713\n",
            "The 200th training loss is1376508533.8140385\n",
            "The 201th training loss is1370268440.5701716\n",
            "The 202th training loss is1364153809.5695708\n",
            "The 203th training loss is1358162112.004927\n",
            "The 204th training loss is1352290870.1840713\n",
            "The 205th training loss is1346537656.4934154\n",
            "The 206th training loss is1340900092.382493\n",
            "The 207th training loss is1335375847.3691723\n",
            "The 208th training loss is1329962638.065105\n",
            "The 209th training loss is1324658227.2210138\n",
            "The 210th training loss is1319460422.7914038\n",
            "The 211th training loss is1314367077.0182996\n",
            "The 212th training loss is1309376085.533628\n",
            "The 213th training loss is1304485386.479854\n",
            "The 214th training loss is1299692959.6485097\n",
            "The 215th training loss is1294996825.6362393\n",
            "The 216th training loss is1290395045.018008\n",
            "The 217th training loss is1285885717.537126\n",
            "The 218th training loss is1281466981.3117409\n",
            "The 219th training loss is1277137012.0574613\n",
            "The 220th training loss is1272894022.3257866\n",
            "The 221th training loss is1268736260.7580168\n",
            "The 222th training loss is1264662011.3543303\n",
            "The 223th training loss is1260669592.7577105\n",
            "The 224th training loss is1256757357.5524342\n",
            "The 225th training loss is1252923691.5768077\n",
            "The 226th training loss is1249167013.249873\n",
            "The 227th training loss is1245485772.9117908\n",
            "The 228th training loss is1241878452.1776261\n",
            "The 229th training loss is1238343563.3042593\n",
            "The 230th training loss is1234879648.5701594\n",
            "The 231th training loss is1231485279.6677525\n",
            "The 232th training loss is1228159057.1081343\n",
            "The 233th training loss is1224899609.6378646\n",
            "The 234th training loss is1221705593.6676147\n",
            "The 235th training loss is1218575692.712405\n",
            "The 236th training loss is1215508616.8432162\n",
            "The 237th training loss is1212503102.1497269\n",
            "The 238th training loss is1209557910.2139626\n",
            "The 239th training loss is1206671827.5946229\n",
            "The 240th training loss is1203843665.3218818\n",
            "The 241th training loss is1201072258.4024339\n",
            "The 242th training loss is1198356465.3345897\n",
            "The 243th training loss is1195695167.6332076\n",
            "The 244th training loss is1193087269.3642666\n",
            "The 245th training loss is1190531696.6888804\n",
            "The 246th training loss is1188027397.4165626\n",
            "The 247th training loss is1185573340.5675533\n",
            "The 248th training loss is1183168515.9440255\n",
            "The 249th training loss is1180811933.7099843\n",
            "The 250th training loss is1178502623.979689\n",
            "The 251th training loss is1176239636.4144213\n",
            "The 252th training loss is1174022039.8274276\n",
            "The 253th training loss is1171848921.7968748\n",
            "The 254th training loss is1169719388.286648\n",
            "The 255th training loss is1167632563.2748427\n",
            "The 256th training loss is1165587588.3897815\n",
            "The 257th training loss is1163583622.5534117\n",
            "The 258th training loss is1161619841.6319284\n",
            "The 259th training loss is1159695438.093482\n",
            "The 260th training loss is1157809620.6728172\n",
            "The 261th training loss is1155961614.042712\n",
            "The 262th training loss is1154150658.4920716\n",
            "The 263th training loss is1152376009.6105437\n",
            "The 264th training loss is1150636937.9795253\n",
            "The 265th training loss is1148932728.8694274\n",
            "The 266th training loss is1147262681.943072\n",
            "The 267th training loss is1145626110.9650967\n",
            "The 268th training loss is1144022343.5172453\n",
            "The 269th training loss is1142450720.7194226\n",
            "The 270th training loss is1140910596.9564004\n",
            "The 271th training loss is1139401339.610055\n",
            "The 272th training loss is1137922328.7970266\n",
            "The 273th training loss is1136472957.11169\n",
            "The 274th training loss is1135052629.3743305\n",
            "The 275th training loss is1133660762.384413\n",
            "The 276th training loss is1132296784.678851\n",
            "The 277th training loss is1130960136.2951622\n",
            "The 278th training loss is1129650268.5394232\n",
            "The 279th training loss is1128366643.758915\n",
            "The 280th training loss is1127108735.1193726\n",
            "The 281th training loss is1125876026.38674\n",
            "The 282th training loss is1124668011.7133422\n",
            "The 283th training loss is1123484195.4283812\n",
            "The 284th training loss is1122324091.8326724\n",
            "The 285th training loss is1121187224.997532\n",
            "The 286th training loss is1120073128.5677328\n",
            "The 287th training loss is1118981345.5684445\n",
            "The 288th training loss is1117911428.2160792\n",
            "The 289th training loss is1116862937.7329633\n",
            "The 290th training loss is1115835444.1657536\n",
            "The 291th training loss is1114828526.2075286\n",
            "The 292th training loss is1113841771.0234754\n",
            "The 293th training loss is1112874774.0801003\n",
            "The 294th training loss is1111927138.977893\n",
            "The 295th training loss is1110998477.287374\n",
            "The 296th training loss is1110088408.388453\n",
            "The 297th training loss is1109196559.313038\n",
            "The 298th training loss is1108322564.590823\n",
            "The 299th training loss is1107466066.0981936\n",
            "The 300th training loss is1106626712.910184\n",
            "The 301th training loss is1105804161.1554272\n",
            "The 302th training loss is1104998073.874036\n",
            "The 303th training loss is1104208120.8783543\n",
            "The 304th training loss is1103433978.6165223\n",
            "The 305th training loss is1102675330.0387967\n",
            "The 306th training loss is1101931864.466574\n",
            "The 307th training loss is1101203277.4640565\n",
            "The 308th training loss is1100489270.712511\n",
            "The 309th training loss is1099789551.8870678\n",
            "The 310th training loss is1099103834.536009\n",
            "The 311th training loss is1098431837.9624894\n",
            "The 312th training loss is1097773287.1086514\n",
            "The 313th training loss is1097127912.4420772\n",
            "The 314th training loss is1096495449.8445334\n",
            "The 315th training loss is1095875640.5029612\n",
            "The 316th training loss is1095268230.8026695\n",
            "The 317th training loss is1094672972.2226813\n",
            "The 318th training loss is1094089621.233194\n",
            "The 319th training loss is1093517939.19511\n",
            "The 320th training loss is1092957692.2615912\n",
            "The 321th training loss is1092408651.2816067\n",
            "The 322th training loss is1091870591.70542\n",
            "The 323th training loss is1091343293.4919887\n",
            "The 324th training loss is1090826541.01823\n",
            "The 325th training loss is1090320122.9901152\n",
            "The 326th training loss is1089823832.3555636\n",
            "The 327th training loss is1089337466.2190866\n",
            "The 328th training loss is1088860825.7581594\n",
            "The 329th training loss is1088393716.141276\n",
            "The 330th training loss is1087935946.4476578\n",
            "The 331th training loss is1087487329.5885835\n",
            "The 332th training loss is1087047682.2303033\n",
            "The 333th training loss is1086616824.7185073\n",
            "The 334th training loss is1086194581.0043206\n",
            "The 335th training loss is1085780778.5717857\n",
            "The 336th training loss is1085375248.3668113\n",
            "The 337th training loss is1084977824.72755\n",
            "The 338th training loss is1084588345.3161838\n",
            "The 339th training loss is1084206651.0520802\n",
            "The 340th training loss is1083832586.046299\n",
            "The 341th training loss is1083465997.5374184\n",
            "The 342th training loss is1083106735.828653\n",
            "The 343th training loss is1082754654.2262428\n",
            "The 344th training loss is1082409608.9790783\n",
            "The 345th training loss is1082071459.2195494\n",
            "The 346th training loss is1081740066.9055786\n",
            "The 347th training loss is1081415296.7638302\n",
            "The 348th training loss is1081097016.2340548\n",
            "The 349th training loss is1080785095.4145644\n",
            "The 350th training loss is1080479407.0087984\n",
            "The 351th training loss is1080179826.2729714\n",
            "The 352th training loss is1079886230.9647717\n",
            "The 353th training loss is1079598501.2930965\n",
            "The 354th training loss is1079316519.868796\n",
            "The 355th training loss is1079040171.656413\n",
            "The 356th training loss is1078769343.9268925\n",
            "The 357th training loss is1078503926.2112436\n",
            "The 358th training loss is1078243810.2551363\n",
            "The 359th training loss is1077988889.9744086\n",
            "The 360th training loss is1077739061.4114769\n",
            "The 361th training loss is1077494222.6926143\n",
            "The 362th training loss is1077254273.9860961\n",
            "The 363th training loss is1077019117.4611878\n",
            "The 364th training loss is1076788657.2479532\n",
            "The 365th training loss is1076562799.397879\n",
            "The 366th training loss is1076341451.8452837\n",
            "The 367th training loss is1076124524.3695118\n",
            "The 368th training loss is1075911928.5578835\n",
            "The 369th training loss is1075703577.7693896\n",
            "The 370th training loss is1075499387.0991213\n",
            "The 371th training loss is1075299273.3434122\n",
            "The 372th training loss is1075103154.9656823\n",
            "The 373th training loss is1074910952.0629718\n",
            "The 374th training loss is1074722586.3331432\n",
            "The 375th training loss is1074537981.0427496\n",
            "The 376th training loss is1074357060.995544\n",
            "The 377th training loss is1074179752.5016246\n",
            "The 378th training loss is1074005983.3472013\n",
            "The 379th training loss is1073835682.7649693\n",
            "The 380th training loss is1073668781.4050778\n",
            "The 381th training loss is1073505211.3066856\n",
            "The 382th training loss is1073344905.8700845\n",
            "The 383th training loss is1073187799.8293875\n",
            "The 384th training loss is1073033829.2257644\n",
            "The 385th training loss is1072882931.3812137\n",
            "The 386th training loss is1072735044.8728659\n",
            "The 387th training loss is1072590109.5078003\n",
            "The 388th training loss is1072448066.2983704\n",
            "The 389th training loss is1072308857.4380226\n",
            "The 390th training loss is1072172426.2776034\n",
            "The 391th training loss is1072038717.3021431\n",
            "The 392th training loss is1071907676.1081038\n",
            "The 393th training loss is1071779249.3810866\n",
            "The 394th training loss is1071653384.8739879\n",
            "The 395th training loss is1071530031.3855913\n",
            "The 396th training loss is1071409138.7395924\n",
            "The 397th training loss is1071290657.7640461\n",
            "The 398th training loss is1071174540.2712231\n",
            "The 399th training loss is1071060739.0378754\n",
            "The 400th training loss is1070949207.7858945\n",
            "The 401th training loss is1070839901.1633623\n",
            "The 402th training loss is1070732774.7259789\n",
            "The 403th training loss is1070627784.9188663\n",
            "The 404th training loss is1070524889.0587361\n",
            "The 405th training loss is1070424045.3164157\n",
            "The 406th training loss is1070325212.6997267\n",
            "The 407th training loss is1070228351.0367055\n",
            "The 408th training loss is1070133420.9591625\n",
            "The 409th training loss is1070040383.8865697\n",
            "The 410th training loss is1069949202.0102739\n",
            "The 411th training loss is1069859838.2780265\n",
            "The 412th training loss is1069772256.3788218\n",
            "The 413th training loss is1069686420.7280431\n",
            "The 414th training loss is1069602296.452905\n",
            "The 415th training loss is1069519849.3781893\n",
            "The 416th training loss is1069439046.0122641\n",
            "The 417th training loss is1069359853.5333896\n",
            "The 418th training loss is1069282239.7762923\n",
            "The 419th training loss is1069206173.2190136\n",
            "The 420th training loss is1069131622.9700196\n",
            "The 421th training loss is1069058558.7555709\n",
            "The 422th training loss is1068986950.9073464\n",
            "The 423th training loss is1068916770.350313\n",
            "The 424th training loss is1068847988.5908428\n",
            "The 425th training loss is1068780577.7050656\n",
            "The 426th training loss is1068714510.3274547\n",
            "The 427th training loss is1068649759.6396459\n",
            "The 428th training loss is1068586299.3594766\n",
            "The 429th training loss is1068524103.7302471\n",
            "The 430th training loss is1068463147.510196\n",
            "The 431th training loss is1068403405.9621881\n",
            "The 432th training loss is1068344854.8436087\n",
            "The 433th training loss is1068287470.3964603\n",
            "The 434th training loss is1068231229.3376592\n",
            "The 435th training loss is1068176108.8495245\n",
            "The 436th training loss is1068122086.5704604\n",
            "The 437th training loss is1068069140.5858238\n",
            "The 438th training loss is1068017249.4189756\n",
            "The 439th training loss is1067966392.0225122\n",
            "The 440th training loss is1067916547.7696708\n",
            "The 441th training loss is1067867696.4459099\n",
            "The 442th training loss is1067819818.2406566\n",
            "The 443th training loss is1067772893.7392201\n",
            "The 444th training loss is1067726903.9148668\n",
            "The 445th training loss is1067681830.1210561\n",
            "The 446th training loss is1067637654.0838295\n",
            "The 447th training loss is1067594357.8943536\n",
            "The 448th training loss is1067551924.0016133\n",
            "The 449th training loss is1067510335.2052495\n",
            "The 450th training loss is1067469574.6485429\n",
            "The 451th training loss is1067429625.8115357\n",
            "The 452th training loss is1067390472.504294\n",
            "The 453th training loss is1067352098.8603027\n",
            "The 454th training loss is1067314489.3299949\n",
            "The 455th training loss is1067277628.6744101\n",
            "The 456th training loss is1067241501.9589788\n",
            "The 457th training loss is1067206094.5474335\n",
            "The 458th training loss is1067171392.0958391\n",
            "The 459th training loss is1067137380.5467471\n",
            "The 460th training loss is1067104046.123461\n",
            "The 461th training loss is1067071375.324423\n",
            "The 462th training loss is1067039354.9177083\n",
            "The 463th training loss is1067007971.9356326\n",
            "The 464th training loss is1066977213.6694663\n",
            "The 465th training loss is1066947067.6642548\n",
            "The 466th training loss is1066917521.7137434\n",
            "The 467th training loss is1066888563.8554025\n",
            "The 468th training loss is1066860182.3655537\n",
            "The 469th training loss is1066832365.7545929\n",
            "The 470th training loss is1066805102.7623092\n",
            "The 471th training loss is1066778382.3532982\n",
            "The 472th training loss is1066752193.7124655\n",
            "The 473th training loss is1066726526.2406228\n",
            "The 474th training loss is1066701369.550169\n",
            "The 475th training loss is1066676713.4608618\n",
            "The 476th training loss is1066652547.9956703\n",
            "The 477th training loss is1066628863.3767115\n",
            "The 478th training loss is1066605650.0212716\n",
            "The 479th training loss is1066582898.5379014\n",
            "The 480th training loss is1066560599.7225939\n",
            "The 481th training loss is1066538744.5550379\n",
            "The 482th training loss is1066517324.1949433\n",
            "The 483th training loss is1066496329.9784471\n",
            "The 484th training loss is1066475753.414583\n",
            "The 485th training loss is1066455586.1818268\n",
            "The 486th training loss is1066435820.1247107\n",
            "The 487th training loss is1066416447.2505033\n",
            "The 488th training loss is1066397459.725958\n",
            "The 489th training loss is1066378849.874125\n",
            "The 490th training loss is1066360610.1712278\n",
            "The 491th training loss is1066342733.2436041\n",
            "The 492th training loss is1066325211.864703\n",
            "The 493th training loss is1066308038.9521476\n",
            "The 494th training loss is1066291207.564854\n",
            "The 495th training loss is1066274710.900207\n",
            "The 496th training loss is1066258542.291295\n",
            "The 497th training loss is1066242695.2041969\n",
            "The 498th training loss is1066227163.2353274\n",
            "The 499th training loss is1066211940.1088305\n",
            "The 500th training loss is1066197019.6740301\n",
            "The 501th training loss is1066182395.9029274\n",
            "The 502th training loss is1066168062.8877524\n",
            "The 503th training loss is1066154014.8385597\n",
            "The 504th training loss is1066140246.0808772\n",
            "The 505th training loss is1066126751.0533988\n",
            "The 506th training loss is1066113524.3057234\n",
            "The 507th training loss is1066100560.4961406\n",
            "The 508th training loss is1066087854.3894598\n",
            "The 509th training loss is1066075400.8548831\n",
            "The 510th training loss is1066063194.863919\n",
            "The 511th training loss is1066051231.4883417\n",
            "The 512th training loss is1066039505.8981869\n",
            "The 513th training loss is1066028013.3597897\n",
            "The 514th training loss is1066016749.2338637\n",
            "The 515th training loss is1066005708.973614\n",
            "The 516th training loss is1065994888.122891\n",
            "The 517th training loss is1065984282.3143816\n",
            "The 518th training loss is1065973887.267835\n",
            "The 519th training loss is1065963698.7883242\n",
            "The 520th training loss is1065953712.7645425\n",
            "The 521th training loss is1065943925.1671343\n",
            "The 522th training loss is1065934332.0470592\n",
            "The 523th training loss is1065924929.5339885\n",
            "The 524th training loss is1065915713.8347343\n",
            "The 525th training loss is1065906681.23171\n",
            "The 526th training loss is1065897828.081419\n",
            "The 527th training loss is1065889150.8129793\n",
            "The 528th training loss is1065880645.9266715\n",
            "The 529th training loss is1065872309.9925194\n",
            "The 530th training loss is1065864139.6488986\n",
            "The 531th training loss is1065856131.6011708\n",
            "The 532th training loss is1065848282.6203492\n",
            "The 533th training loss is1065840589.5417866\n",
            "The 534th training loss is1065833049.2638913\n",
            "The 535th training loss is1065825658.7468709\n",
            "The 536th training loss is1065818415.0114963\n",
            "The 537th training loss is1065811315.137896\n",
            "The 538th training loss is1065804356.264369\n",
            "The 539th training loss is1065797535.5862268\n",
            "The 540th training loss is1065790850.354654\n",
            "The 541th training loss is1065784297.8755946\n",
            "The 542th training loss is1065777875.508659\n",
            "The 543th training loss is1065771580.666053\n",
            "The 544th training loss is1065765410.8115298\n",
            "The 545th training loss is1065759363.4593604\n",
            "The 546th training loss is1065753436.1733267\n",
            "The 547th training loss is1065747626.5657332\n",
            "The 548th training loss is1065741932.2964402\n",
            "The 549th training loss is1065736351.0719138\n",
            "The 550th training loss is1065730880.6442974\n",
            "The 551th training loss is1065725518.8105\n",
            "The 552th training loss is1065720263.4113042\n",
            "The 553th training loss is1065715112.3304904\n",
            "The 554th training loss is1065710063.4939792\n",
            "The 555th training loss is1065705114.8689922\n",
            "The 556th training loss is1065700264.4632266\n",
            "The 557th training loss is1065695510.3240496\n",
            "The 558th training loss is1065690850.5377067\n",
            "The 559th training loss is1065686283.2285466\n",
            "The 560th training loss is1065681806.5582606\n",
            "The 561th training loss is1065677418.7251396\n",
            "The 562th training loss is1065673117.9633434\n",
            "The 563th training loss is1065668902.5421853\n",
            "The 564th training loss is1065664770.7654321\n",
            "The 565th training loss is1065660720.9706163\n",
            "The 566th training loss is1065656751.5283636\n",
            "The 567th training loss is1065652860.8417331\n",
            "The 568th training loss is1065649047.3455704\n",
            "The 569th training loss is1065645309.5058738\n",
            "The 570th training loss is1065641645.8191748\n",
            "The 571th training loss is1065638054.8119268\n",
            "The 572th training loss is1065634535.0399122\n",
            "The 573th training loss is1065631085.087654\n",
            "The 574th training loss is1065627703.5678456\n",
            "The 575th training loss is1065624389.120789\n",
            "The 576th training loss is1065621140.4138436\n",
            "The 577th training loss is1065617956.1408877\n",
            "The 578th training loss is1065614835.02179\n",
            "The 579th training loss is1065611775.8018905\n",
            "The 580th training loss is1065608777.2514956\n",
            "The 581th training loss is1065605838.1653773\n",
            "The 582th training loss is1065602957.3622884\n",
            "The 583th training loss is1065600133.6844827\n",
            "The 584th training loss is1065597365.9972479\n",
            "The 585th training loss is1065594653.1884469\n",
            "The 586th training loss is1065591994.1680666\n",
            "The 587th training loss is1065589387.867778\n",
            "The 588th training loss is1065586833.2405049\n",
            "The 589th training loss is1065584329.2599989\n",
            "The 590th training loss is1065581874.920426\n",
            "The 591th training loss is1065579469.2359589\n",
            "The 592th training loss is1065577111.240379\n",
            "The 593th training loss is1065574799.9866849\n",
            "The 594th training loss is1065572534.5467118\n",
            "The 595th training loss is1065570314.0107528\n",
            "The 596th training loss is1065568137.4871951\n",
            "The 597th training loss is1065566004.1021568\n",
            "The 598th training loss is1065563912.9991349\n",
            "The 599th training loss is1065561863.3386586\n",
            "The 600th training loss is1065559854.2979517\n",
            "The 601th training loss is1065557885.0705972\n",
            "The 602th training loss is1065555954.8662151\n",
            "The 603th training loss is1065554062.9101393\n",
            "The 604th training loss is1065552208.4431081\n",
            "The 605th training loss is1065550390.720955\n",
            "The 606th training loss is1065548609.0143089\n",
            "The 607th training loss is1065546862.6083006\n",
            "The 608th training loss is1065545150.802272\n",
            "The 609th training loss is1065543472.9094951\n",
            "The 610th training loss is1065541828.256893\n",
            "The 611th training loss is1065540216.1847693\n",
            "The 612th training loss is1065538636.0465415\n",
            "The 613th training loss is1065537087.2084789\n",
            "The 614th training loss is1065535569.0494485\n",
            "The 615th training loss is1065534080.9606633\n",
            "The 616th training loss is1065532622.3454354\n",
            "The 617th training loss is1065531192.6189375\n",
            "The 618th training loss is1065529791.2079651\n",
            "The 619th training loss is1065528417.5507058\n",
            "The 620th training loss is1065527071.0965121\n",
            "The 621th training loss is1065525751.3056799\n",
            "The 622th training loss is1065524457.6492304\n",
            "The 623th training loss is1065523189.6086953\n",
            "The 624th training loss is1065521946.6759105\n",
            "The 625th training loss is1065520728.3528082\n",
            "The 626th training loss is1065519534.1512179\n",
            "The 627th training loss is1065518363.5926683\n",
            "The 628th training loss is1065517216.2081945\n",
            "The 629th training loss is1065516091.5381495\n",
            "The 630th training loss is1065514989.1320184\n",
            "The 631th training loss is1065513908.5482358\n",
            "The 632th training loss is1065512849.3540092\n",
            "The 633th training loss is1065511811.1251438\n",
            "The 634th training loss is1065510793.4458712\n",
            "The 635th training loss is1065509795.908682\n",
            "The 636th training loss is1065508818.1141616\n",
            "The 637th training loss is1065507859.670829\n",
            "The 638th training loss is1065506920.1949791\n",
            "The 639th training loss is1065505999.3105277\n",
            "The 640th training loss is1065505096.6488601\n",
            "The 641th training loss is1065504211.8486825\n",
            "The 642th training loss is1065503344.5558772\n",
            "The 643th training loss is1065502494.4233581\n",
            "The 644th training loss is1065501661.1109322\n",
            "The 645th training loss is1065500844.2851621\n",
            "The 646th training loss is1065500043.6192322\n",
            "The 647th training loss is1065499258.7928151\n",
            "The 648th training loss is1065498489.4919455\n",
            "The 649th training loss is1065497735.4088908\n",
            "The 650th training loss is1065496996.2420299\n",
            "The 651th training loss is1065496271.6957284\n",
            "The 652th training loss is1065495561.4802233\n",
            "The 653th training loss is1065494865.3115029\n",
            "The 654th training loss is1065494182.9111944\n",
            "The 655th training loss is1065493514.0064514\n",
            "The 656th training loss is1065492858.3298438\n",
            "The 657th training loss is1065492215.6192497\n",
            "The 658th training loss is1065491585.617751\n",
            "The 659th training loss is1065490968.073529\n",
            "The 660th training loss is1065490362.739763\n",
            "The 661th training loss is1065489769.3745317\n",
            "The 662th training loss is1065489187.7407157\n",
            "The 663th training loss is1065488617.6059006\n",
            "The 664th training loss is1065488058.7422854\n",
            "The 665th training loss is1065487510.9265894\n",
            "The 666th training loss is1065486973.9399636\n",
            "The 667th training loss is1065486447.5679016\n",
            "The 668th training loss is1065485931.6001531\n",
            "The 669th training loss is1065485425.8306404\n",
            "The 670th training loss is1065484930.0573752\n",
            "The 671th training loss is1065484444.0823758\n",
            "The 672th training loss is1065483967.7115903\n",
            "The 673th training loss is1065483500.7548155\n",
            "The 674th training loss is1065483043.0256225\n",
            "The 675th training loss is1065482594.341281\n",
            "The 676th training loss is1065482154.5226854\n",
            "The 677th training loss is1065481723.3942838\n",
            "The 678th training loss is1065481300.7840062\n",
            "The 679th training loss is1065480886.5231963\n",
            "The 680th training loss is1065480480.4465432\n",
            "The 681th training loss is1065480082.3920146\n",
            "The 682th training loss is1065479692.2007923\n",
            "The 683th training loss is1065479309.7172086\n",
            "The 684th training loss is1065478934.7886825\n",
            "The 685th training loss is1065478567.2656595\n",
            "The 686th training loss is1065478207.0015509\n",
            "The 687th training loss is1065477853.8526751\n",
            "The 688th training loss is1065477507.6782001\n",
            "The 689th training loss is1065477168.340086\n",
            "The 690th training loss is1065476835.7030305\n",
            "The 691th training loss is1065476509.6344147\n",
            "The 692th training loss is1065476190.0042483\n",
            "The 693th training loss is1065475876.6851188\n",
            "The 694th training loss is1065475569.5521404\n",
            "The 695th training loss is1065475268.4829022\n",
            "The 696th training loss is1065474973.3574214\n",
            "The 697th training loss is1065474684.0580927\n",
            "The 698th training loss is1065474400.469643\n",
            "The 699th training loss is1065474122.4790841\n",
            "The 700th training loss is1065473849.9756675\n",
            "The 701th training loss is1065473582.8508393\n",
            "The 702th training loss is1065473320.9981986\n",
            "The 703th training loss is1065473064.3134527\n",
            "The 704th training loss is1065472812.6943762\n",
            "The 705th training loss is1065472566.0407693\n",
            "The 706th training loss is1065472324.2544184\n",
            "The 707th training loss is1065472087.2390563\n",
            "The 708th training loss is1065471854.9003233\n",
            "The 709th training loss is1065471627.1457303\n",
            "The 710th training loss is1065471403.8846197\n",
            "The 711th training loss is1065471185.0281317\n",
            "The 712th training loss is1065470970.4891663\n",
            "The 713th training loss is1065470760.1823497\n",
            "The 714th training loss is1065470554.023999\n",
            "The 715th training loss is1065470351.9320904\n",
            "The 716th training loss is1065470153.8262246\n",
            "The 717th training loss is1065469959.6275945\n",
            "The 718th training loss is1065469769.2589546\n",
            "The 719th training loss is1065469582.6445901\n",
            "The 720th training loss is1065469399.7102858\n",
            "The 721th training loss is1065469220.3832961\n",
            "The 722th training loss is1065469044.5923165\n",
            "The 723th training loss is1065468872.2674555\n",
            "The 724th training loss is1065468703.3402047\n",
            "The 725th training loss is1065468537.7434137\n",
            "The 726th training loss is1065468375.4112613\n",
            "The 727th training loss is1065468216.2792302\n",
            "The 728th training loss is1065468060.2840806\n",
            "The 729th training loss is1065467907.3638242\n",
            "The 730th training loss is1065467757.4577022\n",
            "The 731th training loss is1065467610.5061573\n",
            "The 732th training loss is1065467466.4508113\n",
            "The 733th training loss is1065467325.2344432\n",
            "The 734th training loss is1065467186.8009635\n",
            "The 735th training loss is1065467051.095394\n",
            "The 736th training loss is1065466918.063845\n",
            "The 737th training loss is1065466787.6534934\n",
            "The 738th training loss is1065466659.8125616\n",
            "The 739th training loss is1065466534.4902976\n",
            "The 740th training loss is1065466411.6369529\n",
            "The 741th training loss is1065466291.2037646\n",
            "The 742th training loss is1065466173.1429352\n",
            "The 743th training loss is1065466057.4076124\n",
            "The 744th training loss is1065465943.9518721\n",
            "The 745th training loss is1065465832.7306982\n",
            "The 746th training loss is1065465723.6999658\n",
            "The 747th training loss is1065465616.8164234\n",
            "The 748th training loss is1065465512.0376756\n",
            "The 749th training loss is1065465409.3221651\n",
            "The 750th training loss is1065465308.6291575\n",
            "The 751th training loss is1065465209.9187245\n",
            "The 752th training loss is1065465113.1517276\n",
            "The 753th training loss is1065465018.2898024\n",
            "The 754th training loss is1065464925.2953441\n",
            "The 755th training loss is1065464834.1314914\n",
            "The 756th training loss is1065464744.7621124\n",
            "The 757th training loss is1065464657.1517897\n",
            "The 758th training loss is1065464571.2658068\n",
            "The 759th training loss is1065464487.070134\n",
            "The 760th training loss is1065464404.5314143\n",
            "The 761th training loss is1065464323.6169511\n",
            "The 762th training loss is1065464244.2946936\n",
            "The 763th training loss is1065464166.5332253\n",
            "The 764th training loss is1065464090.3017514\n",
            "The 765th training loss is1065464015.5700855\n",
            "The 766th training loss is1065463942.3086387\n",
            "The 767th training loss is1065463870.4884063\n",
            "The 768th training loss is1065463800.0809584\n",
            "The 769th training loss is1065463731.0584261\n",
            "The 770th training loss is1065463663.393492\n",
            "The 771th training loss is1065463597.0593789\n",
            "The 772th training loss is1065463532.0298387\n",
            "The 773th training loss is1065463468.2791426\n",
            "The 774th training loss is1065463405.7820696\n",
            "The 775th training loss is1065463344.5138986\n",
            "The 776th training loss is1065463284.4503962\n",
            "The 777th training loss is1065463225.5678082\n",
            "The 778th training loss is1065463167.84285\n",
            "The 779th training loss is1065463111.2526966\n",
            "The 780th training loss is1065463055.7749755\n",
            "The 781th training loss is1065463001.387755\n",
            "The 782th training loss is1065462948.0695374\n",
            "The 783th training loss is1065462895.7992501\n",
            "The 784th training loss is1065462844.5562361\n",
            "The 785th training loss is1065462794.3202477\n",
            "The 786th training loss is1065462745.0714358\n",
            "The 787th training loss is1065462696.7903448\n",
            "The 788th training loss is1065462649.4579033\n",
            "The 789th training loss is1065462603.0554157\n",
            "The 790th training loss is1065462557.5645568\n",
            "The 791th training loss is1065462512.9673634\n",
            "The 792th training loss is1065462469.2462261\n",
            "The 793th training loss is1065462426.3838844\n",
            "The 794th training loss is1065462384.3634181\n",
            "The 795th training loss is1065462343.1682421\n",
            "The 796th training loss is1065462302.7820978\n",
            "The 797th training loss is1065462263.189048\n",
            "The 798th training loss is1065462224.3734705\n",
            "The 799th training loss is1065462186.3200517\n",
            "The 800th training loss is1065462149.0137804\n",
            "The 801th training loss is1065462112.4399409\n",
            "The 802th training loss is1065462076.5841094\n",
            "The 803th training loss is1065462041.4321461\n",
            "The 804th training loss is1065462006.9701902\n",
            "The 805th training loss is1065461973.1846555\n",
            "The 806th training loss is1065461940.0622232\n",
            "The 807th training loss is1065461907.5898374\n",
            "The 808th training loss is1065461875.7547005\n",
            "The 809th training loss is1065461844.5442672\n",
            "The 810th training loss is1065461813.94624\n",
            "The 811th training loss is1065461783.9485632\n",
            "The 812th training loss is1065461754.5394207\n",
            "The 813th training loss is1065461725.7072282\n",
            "The 814th training loss is1065461697.4406308\n",
            "The 815th training loss is1065461669.7284973\n",
            "The 816th training loss is1065461642.5599158\n",
            "The 817th training loss is1065461615.924191\n",
            "The 818th training loss is1065461589.810837\n",
            "The 819th training loss is1065461564.2095764\n",
            "The 820th training loss is1065461539.110333\n",
            "The 821th training loss is1065461514.5032309\n",
            "The 822th training loss is1065461490.3785875\n",
            "The 823th training loss is1065461466.7269129\n",
            "The 824th training loss is1065461443.5389026\n",
            "The 825th training loss is1065461420.8054371\n",
            "The 826th training loss is1065461398.517576\n",
            "The 827th training loss is1065461376.6665555\n",
            "The 828th training loss is1065461355.2437849\n",
            "The 829th training loss is1065461334.2408417\n",
            "The 830th training loss is1065461313.6494715\n",
            "The 831th training loss is1065461293.4615809\n",
            "The 832th training loss is1065461273.6692371\n",
            "The 833th training loss is1065461254.2646632\n",
            "The 834th training loss is1065461235.240236\n",
            "The 835th training loss is1065461216.5884819\n",
            "The 836th training loss is1065461198.302076\n",
            "The 837th training loss is1065461180.3738359\n",
            "The 838th training loss is1065461162.7967229\n",
            "The 839th training loss is1065461145.5638355\n",
            "The 840th training loss is1065461128.6684088\n",
            "The 841th training loss is1065461112.1038108\n",
            "The 842th training loss is1065461095.8635412\n",
            "The 843th training loss is1065461079.941227\n",
            "The 844th training loss is1065461064.3306205\n",
            "The 845th training loss is1065461049.0255982\n",
            "The 846th training loss is1065461034.0201566\n",
            "The 847th training loss is1065461019.3084102\n",
            "The 848th training loss is1065461004.8845905\n",
            "The 849th training loss is1065460990.7430407\n",
            "The 850th training loss is1065460976.8782182\n",
            "The 851th training loss is1065460963.2846872\n",
            "The 852th training loss is1065460949.9571203\n",
            "The 853th training loss is1065460936.8902943\n",
            "The 854th training loss is1065460924.0790896\n",
            "The 855th training loss is1065460911.5184867\n",
            "The 856th training loss is1065460899.2035656\n",
            "The 857th training loss is1065460887.1295028\n",
            "The 858th training loss is1065460875.29157\n",
            "The 859th training loss is1065460863.6851315\n",
            "The 860th training loss is1065460852.305644\n",
            "The 861th training loss is1065460841.1486524\n",
            "The 862th training loss is1065460830.20979\n",
            "The 863th training loss is1065460819.4847759\n",
            "The 864th training loss is1065460808.9694139\n",
            "The 865th training loss is1065460798.6595894\n",
            "The 866th training loss is1065460788.55127\n",
            "The 867th training loss is1065460778.6405019\n",
            "The 868th training loss is1065460768.9234092\n",
            "The 869th training loss is1065460759.3961926\n",
            "The 870th training loss is1065460750.0551273\n",
            "The 871th training loss is1065460740.8965621\n",
            "The 872th training loss is1065460731.916917\n",
            "The 873th training loss is1065460723.1126832\n",
            "The 874th training loss is1065460714.4804207\n",
            "The 875th training loss is1065460706.0167567\n",
            "The 876th training loss is1065460697.7183855\n",
            "The 877th training loss is1065460689.5820662\n",
            "The 878th training loss is1065460681.6046209\n",
            "The 879th training loss is1065460673.782936\n",
            "The 880th training loss is1065460666.113957\n",
            "The 881th training loss is1065460658.594691\n",
            "The 882th training loss is1065460651.2222035\n",
            "The 883th training loss is1065460643.9936172\n",
            "The 884th training loss is1065460636.9061124\n",
            "The 885th training loss is1065460629.9569242\n",
            "The 886th training loss is1065460623.1433419\n",
            "The 887th training loss is1065460616.4627086\n",
            "The 888th training loss is1065460609.9124192\n",
            "The 889th training loss is1065460603.4899203\n",
            "The 890th training loss is1065460597.1927083\n",
            "The 891th training loss is1065460591.0183289\n",
            "The 892th training loss is1065460584.9643762\n",
            "The 893th training loss is1065460579.0284915\n",
            "The 894th training loss is1065460573.2083621\n",
            "The 895th training loss is1065460567.5017215\n",
            "The 896th training loss is1065460561.9063474\n",
            "The 897th training loss is1065460556.4200605\n",
            "The 898th training loss is1065460551.0407255\n",
            "The 899th training loss is1065460545.7662483\n",
            "The 900th training loss is1065460540.5945762\n",
            "The 901th training loss is1065460535.5236967\n",
            "The 902th training loss is1065460530.5516363\n",
            "The 903th training loss is1065460525.6764623\n",
            "The 904th training loss is1065460520.8962771\n",
            "The 905th training loss is1065460516.2092227\n",
            "The 906th training loss is1065460511.613476\n",
            "The 907th training loss is1065460507.1072507\n",
            "The 908th training loss is1065460502.6887956\n",
            "The 909th training loss is1065460498.3563929\n",
            "The 910th training loss is1065460494.1083598\n",
            "The 911th training loss is1065460489.9430459\n",
            "The 912th training loss is1065460485.8588328\n",
            "The 913th training loss is1065460481.8541343\n",
            "The 914th training loss is1065460477.9273959\n",
            "The 915th training loss is1065460474.0770929\n",
            "The 916th training loss is1065460470.3017303\n",
            "The 917th training loss is1065460466.5998433\n",
            "The 918th training loss is1065460462.9699949\n",
            "The 919th training loss is1065460459.4107771\n",
            "The 920th training loss is1065460455.9208094\n",
            "The 921th training loss is1065460452.4987379\n",
            "The 922th training loss is1065460449.1432356\n",
            "The 923th training loss is1065460445.8530018\n",
            "The 924th training loss is1065460442.6267604\n",
            "The 925th training loss is1065460439.4632615\n",
            "The 926th training loss is1065460436.3612792\n",
            "The 927th training loss is1065460433.3196111\n",
            "The 928th training loss is1065460430.3370796\n",
            "The 929th training loss is1065460427.4125286\n",
            "The 930th training loss is1065460424.5448267\n",
            "The 931th training loss is1065460421.732863\n",
            "The 932th training loss is1065460418.9755487\n",
            "The 933th training loss is1065460416.2718167\n",
            "The 934th training loss is1065460413.620621\n",
            "The 935th training loss is1065460411.0209359\n",
            "The 936th training loss is1065460408.4717554\n",
            "The 937th training loss is1065460405.9720936\n",
            "The 938th training loss is1065460403.5209839\n",
            "The 939th training loss is1065460401.1174785\n",
            "The 940th training loss is1065460398.7606485\n",
            "The 941th training loss is1065460396.4495833\n",
            "The 942th training loss is1065460394.1833893\n",
            "The 943th training loss is1065460391.961191\n",
            "The 944th training loss is1065460389.7821304\n",
            "The 945th training loss is1065460387.6453652\n",
            "The 946th training loss is1065460385.5500712\n",
            "The 947th training loss is1065460383.4954387\n",
            "The 948th training loss is1065460381.4806751\n",
            "The 949th training loss is1065460379.5050027\n",
            "The 950th training loss is1065460377.5676594\n",
            "The 951th training loss is1065460375.6678972\n",
            "The 952th training loss is1065460373.8049839\n",
            "The 953th training loss is1065460371.978201\n",
            "The 954th training loss is1065460370.1868441\n",
            "The 955th training loss is1065460368.4302226\n",
            "The 956th training loss is1065460366.7076594\n",
            "The 957th training loss is1065460365.0184909\n",
            "The 958th training loss is1065460363.3620658\n",
            "The 959th training loss is1065460361.7377467\n",
            "The 960th training loss is1065460360.144908\n",
            "The 961th training loss is1065460358.5829362\n",
            "The 962th training loss is1065460357.0512296\n",
            "The 963th training loss is1065460355.5491991\n",
            "The 964th training loss is1065460354.0762668\n",
            "The 965th training loss is1065460352.6318656\n",
            "The 966th training loss is1065460351.2154399\n",
            "The 967th training loss is1065460349.8264451\n",
            "The 968th training loss is1065460348.4643471\n",
            "The 969th training loss is1065460347.1286224\n",
            "The 970th training loss is1065460345.8187574\n",
            "The 971th training loss is1065460344.5342485\n",
            "The 972th training loss is1065460343.2746027\n",
            "The 973th training loss is1065460342.0393355\n",
            "The 974th training loss is1065460340.8279729\n",
            "The 975th training loss is1065460339.6400496\n",
            "The 976th training loss is1065460338.4751096\n",
            "The 977th training loss is1065460337.332706\n",
            "The 978th training loss is1065460336.2123998\n",
            "The 979th training loss is1065460335.1137618\n",
            "The 980th training loss is1065460334.0363703\n",
            "The 981th training loss is1065460332.9798115\n",
            "The 982th training loss is1065460331.9436817\n",
            "The 983th training loss is1065460330.9275824\n",
            "The 984th training loss is1065460329.9311248\n",
            "The 985th training loss is1065460328.9539266\n",
            "The 986th training loss is1065460327.9956138\n",
            "The 987th training loss is1065460327.0558194\n",
            "The 988th training loss is1065460326.134183\n",
            "The 989th training loss is1065460325.2303525\n",
            "The 990th training loss is1065460324.343981\n",
            "The 991th training loss is1065460323.4747298\n",
            "The 992th training loss is1065460322.6222664\n",
            "The 993th training loss is1065460321.7862641\n",
            "The 994th training loss is1065460320.9664041\n",
            "The 995th training loss is1065460320.1623719\n",
            "The 996th training loss is1065460319.3738611\n",
            "The 997th training loss is1065460318.6005694\n",
            "The 998th training loss is1065460317.8422017\n",
            "The 999th training loss is1065460317.0984681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 7] Learning curve plot"
      ],
      "metadata": {
        "id": "FmGP0V-M1No9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(slr.loss)\n",
        "plt.plot(slr.val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "dhHVBvVE09ts",
        "outputId": "188967a3-3544-4cf5-f097-b3588eb87a06"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f196b073ee0>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}